{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:13.959324Z",
     "start_time": "2025-11-13T08:06:07.204893Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "from itertools import islice\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import pipeline, CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "LOADER_PATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Cuda Availability:{torch.cuda.is_available()} Training on {device}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukun\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Availability:True Training on cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c24885366b00e5e9",
   "metadata": {},
   "source": [
    "# Notebook for interactive testing for CLIP"
   ]
  },
  {
   "cell_type": "code",
   "id": "afdd8de3917c931a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:18.508618Z",
     "start_time": "2025-11-13T08:06:13.959324Z"
    }
   },
   "source": [
    "\n",
    "class Cfg:\n",
    "    model_id: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    seed:   int = 42\n",
    "\n",
    "    # -------- Optim & Loss ----------\n",
    "    lr_head: float = 1e-3\n",
    "    wd_head: float = 1e-4\n",
    "    lr_lora: float = 1e-4 \n",
    "    wd_lora: float = 1e-2\n",
    "    lambda_text: float = 0.3\n",
    "    \n",
    "    early_stopping: bool = True\n",
    "    early_stop_patience = 3\n",
    "    early_stop_minimum_improvement:float = 0.05\n",
    "    \n",
    "\n",
    "    # -------- LoRA ----------\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_target: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")\n",
    "\n",
    "\n",
    "    amp: bool = True\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "PREPROCESS_DATA_ROOT = \"../data/preprocessed\"\n",
    "torch.manual_seed(cfg.seed)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device).eval()\n",
    "processor  = CLIPProcessor.from_pretrained(model_id)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yukun\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1357: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  return t.to(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2a9dfdb42fcf2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:20.508380Z",
     "start_time": "2025-11-13T08:06:18.508618Z"
    }
   },
   "source": [
    "# ---------- 2. Load Dataset ----------\n",
    "def collate_pil(batch):\n",
    "    # batch: List[ (PIL.Image.Image, int) ]\n",
    "    images, labels = zip(*batch)           # images: tuple of PIL, labels: tuple of int\n",
    "    return list(images), torch.tensor(labels)\n",
    "\n",
    "def preprocess_dataset(split=\"train\",data_root='../data',outputdir=\"../data/preprocessed\",batchsize=cfg.batch_size,fp16=False):\n",
    "    os.makedirs(outputdir,exist_ok=True)\n",
    "    dataset = datasets.Flowers102(root=data_root, split=split, download=True)\n",
    "    classes = dataset.classes\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle= True if split==\"train\" else False, num_workers=0,collate_fn=collate_pil)\n",
    "    \n",
    "    # Allocate RAM\n",
    "    N = len(dataset)\n",
    "    C, H, W = 3, 224, 224\n",
    "    pixels = torch.empty((N,C,H,W), dtype=torch.float32 if not fp16 else torch.float16)\n",
    "    labels = torch.empty(N, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Preprocessing {split} data...\")\n",
    "    \n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for images, y in tqdm(loader,desc=f\"Preprocessing {split}\"):\n",
    "            pix = processor(images=images,return_tensors=\"pt\")['pixel_values']\n",
    "            if fp16:\n",
    "                pix = pix.half()\n",
    "            b = pix.size(0) # patch size\n",
    "            pixels[index:index+b] = pix\n",
    "            labels[index:index+b] = y\n",
    "            index += b\n",
    "    \n",
    "    \n",
    "    out_path = os.path.join(outputdir, f\"{split}.pt\")\n",
    "    torch.save({\"pixel_values\": pixels, \"labels\": labels, \"fp16\": fp16,\"classes\":classes}, out_path)\n",
    "    print(f\"Saved → {out_path} (pixels: {pixels.shape}, dtype={pixels.dtype})\")\n",
    "    \n",
    "    print(\"Performing Garbage Cleaning...\")\n",
    "    del pixels, labels\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # for notebook\n",
    "    %reset_selective -f pixels\n",
    "    %reset_selective -f labels\n",
    "\n",
    "class CacheDataset:\n",
    "    def __init__(self, split=\"train\", root=\"../data/preprocessed\"):\n",
    "        path = os.path.join(root, f\"{split}.pt\")\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "        self.pixel_values = obj[\"pixel_values\"]\n",
    "        self.labels = obj[\"labels\"]\n",
    "        self.fp16 = bool(obj.get(\"fp16\", False))\n",
    "        self.classes = obj['classes']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels.numel()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pixel_values[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "# Preprocess and save dataset                \n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"train.pt\")):    \n",
    "    preprocess_dataset(split=\"train\",data_root=\"../data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"val.pt\")):  \n",
    "    preprocess_dataset(split=\"val\",data_root=\"../data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"test.pt\")):  \n",
    "    preprocess_dataset(split=\"test\",data_root=\"../data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Load dataset\n",
    "train_set = CacheDataset(split=\"train\")\n",
    "val_set = CacheDataset(split=\"val\")\n",
    "test_set = CacheDataset(split=\"test\")\n",
    "\n",
    "classname = val_set.classes\n",
    "classname\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink primrose',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'canterbury bells',\n",
       " 'sweet pea',\n",
       " 'english marigold',\n",
       " 'tiger lily',\n",
       " 'moon orchid',\n",
       " 'bird of paradise',\n",
       " 'monkshood',\n",
       " 'globe thistle',\n",
       " 'snapdragon',\n",
       " \"colt's foot\",\n",
       " 'king protea',\n",
       " 'spear thistle',\n",
       " 'yellow iris',\n",
       " 'globe-flower',\n",
       " 'purple coneflower',\n",
       " 'peruvian lily',\n",
       " 'balloon flower',\n",
       " 'giant white arum lily',\n",
       " 'fire lily',\n",
       " 'pincushion flower',\n",
       " 'fritillary',\n",
       " 'red ginger',\n",
       " 'grape hyacinth',\n",
       " 'corn poppy',\n",
       " 'prince of wales feathers',\n",
       " 'stemless gentian',\n",
       " 'artichoke',\n",
       " 'sweet william',\n",
       " 'carnation',\n",
       " 'garden phlox',\n",
       " 'love in the mist',\n",
       " 'mexican aster',\n",
       " 'alpine sea holly',\n",
       " 'ruby-lipped cattleya',\n",
       " 'cape flower',\n",
       " 'great masterwort',\n",
       " 'siam tulip',\n",
       " 'lenten rose',\n",
       " 'barbeton daisy',\n",
       " 'daffodil',\n",
       " 'sword lily',\n",
       " 'poinsettia',\n",
       " 'bolero deep blue',\n",
       " 'wallflower',\n",
       " 'marigold',\n",
       " 'buttercup',\n",
       " 'oxeye daisy',\n",
       " 'common dandelion',\n",
       " 'petunia',\n",
       " 'wild pansy',\n",
       " 'primula',\n",
       " 'sunflower',\n",
       " 'pelargonium',\n",
       " 'bishop of llandaff',\n",
       " 'gaura',\n",
       " 'geranium',\n",
       " 'orange dahlia',\n",
       " 'pink-yellow dahlia?',\n",
       " 'cautleya spicata',\n",
       " 'japanese anemone',\n",
       " 'black-eyed susan',\n",
       " 'silverbush',\n",
       " 'californian poppy',\n",
       " 'osteospermum',\n",
       " 'spring crocus',\n",
       " 'bearded iris',\n",
       " 'windflower',\n",
       " 'tree poppy',\n",
       " 'gazania',\n",
       " 'azalea',\n",
       " 'water lily',\n",
       " 'rose',\n",
       " 'thorn apple',\n",
       " 'morning glory',\n",
       " 'passion flower',\n",
       " 'lotus',\n",
       " 'toad lily',\n",
       " 'anthurium',\n",
       " 'frangipani',\n",
       " 'clematis',\n",
       " 'hibiscus',\n",
       " 'columbine',\n",
       " 'desert-rose',\n",
       " 'tree mallow',\n",
       " 'magnolia',\n",
       " 'cyclamen',\n",
       " 'watercress',\n",
       " 'canna lily',\n",
       " 'hippeastrum',\n",
       " 'bee balm',\n",
       " 'ball moss',\n",
       " 'foxglove',\n",
       " 'bougainvillea',\n",
       " 'camellia',\n",
       " 'mallow',\n",
       " 'mexican petunia',\n",
       " 'bromelia',\n",
       " 'blanket flower',\n",
       " 'trumpet creeper',\n",
       " 'blackberry lily']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:20.510730Z",
     "start_time": "2025-11-13T08:06:20.508380Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1a8294a5bd088ae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e55416c71fb1fdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:20.514352Z",
     "start_time": "2025-11-13T08:06:20.510730Z"
    }
   },
   "source": [
    "# ---------- 3. DataLoader ----------\n",
    "# ---------- Process of images is put on epoch loops\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True\n",
    "    #workers should be 4, but got problems in notebook\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "ea3ab3c785fcbdf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Setting and Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d9682cb89ba6dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:06:21.531420Z",
     "start_time": "2025-11-13T08:06:20.514352Z"
    }
   },
   "source": [
    "# Word Prompt Embedding\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "promptTemplate = {\n",
    "    \"A photo of {}.\",\n",
    "    \"A photo of flower {}.\",\n",
    "    \"Botanic picture of {}\",\n",
    "    \"A example picture of type {}\"\n",
    "}\n",
    "# Use more templates to reduce sensitivity to other contexts\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(names):\n",
    "    embs = []\n",
    "    for name in tqdm(names, desc=\"TextEmbed\"):\n",
    "        prompts = [t.format(name.replace(\"_\",\" \")) for t in promptTemplate] # insert class names\n",
    "        inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        te = clip_model.get_text_features(**inputs)     # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        embs.append(te.mean(dim=0))                     # [D]\n",
    "    text = torch.stack(embs, dim=0)                     # [C, D]\n",
    "    return text / text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embs = build_text_embeddings(classname)          # [102, D], 固定不训练\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextEmbed: 100%|██████████| 102/102 [00:01<00:00, 101.21it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c5246531ba1768ef",
   "metadata": {},
   "source": [
    "----\n",
    "## Building CLIP model with LoRA and word embedding.\n",
    "\n",
    "Have to implement a LoRA linear layer ourselves."
   ]
  },
  {
   "cell_type": "code",
   "id": "63a9ba3e82edb850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:11:45.064526Z",
     "start_time": "2025-11-13T08:11:45.054266Z"
    }
   },
   "source": [
    "# LoRA Injection (Trains LoRA matries only)\n",
    "import torch, torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "\n",
    "# y = w0 + x*(BA)*alpha/rank\n",
    "# Shape of A: din by rank / Shape of B: rank by dout\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r=8, alpha=16, dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.base = base # linear layer frozen for training LoRA parameters\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "        dev = base.weight.device\n",
    "        dt  = base.weight.dtype\n",
    "\n",
    "        \n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(base.in_features, r, bias=False).to(dev, dtype=dt)\n",
    "            self.lora_B = nn.Linear(r, base.out_features, bias=False).to(dev, dtype=dt)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight,a=5**0.5)\n",
    "            nn.init.zeros_(self.lora_B.weight) # set B to 0, avoid any bias introduced.\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "            #Frozen\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return self.base(x)\n",
    "\n",
    "\n",
    "# LoRA Injection with warped LoRA layer shown above.\n",
    "\n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "    clip_model.to(device)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f3005ec8ad7ff276"
  },
  {
   "cell_type": "code",
   "id": "8a444b54cc7e7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:11:45.814557Z",
     "start_time": "2025-11-13T08:11:45.801262Z"
    }
   },
   "source": [
    "head, optimizer, scaler = build_head_and_optim(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_image_feats(images):\n",
    "    # images are actual tensor now\n",
    "    # assume it's preprocessed\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        pixel_values = images.to(device, dtype=torch.float16 if (device==\"cuda\" and cfg.amp) else torch.float32)\n",
    "    else:\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        \n",
    "    feats = clip_model.get_image_features(pixel_values=pixel_values)           # [B, D]\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "ecf8ad5eafb9cbda",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Main Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "id": "75d9230be58c43ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:11:46.434537Z",
     "start_time": "2025-11-13T08:11:46.418615Z"
    }
   },
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.last_loss = 0\n",
    "        self.patience = cfg.early_stop_patience\n",
    "        self.enable = cfg.early_stopping\n",
    "        self.delta = cfg.early_stop_minimum_improvement\n",
    "    def report(self,loss):\n",
    "        if self.last_loss - loss < self.delta:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        self.last_loss = loss\n",
    "    \n",
    "    def stop_flag(self):\n",
    "        return self.enable and (self.counter >= self.patience)\n",
    "        \n",
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = get_image_feats(images)                   # [B, D]\n",
    "\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "        \n",
    "    loss_avg = loss_sum / total\n",
    "    \n",
    "    return {\n",
    "    \"loss\": loss_avg,\n",
    "    \"acc_cls\": correct_cls/total,\n",
    "    \"acc_txt\": correct_txt/total,\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "11a145942664fd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T08:13:33.067530Z",
     "start_time": "2025-11-13T08:11:46.873161Z"
    }
   },
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "earlystop = EarlyStopper()\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"[{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "    earlystop.report(val['loss'])\n",
    "    if earlystop.stop_flag():\n",
    "        print(f\"Early stop triggered...Exiting on epoch {ep}\")\n",
    "        break\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 16.40it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/40] Train: loss=4.5103 acc_cls=0.5324 acc_txt=1.0000 | Val:   loss=4.7500 acc_cls=0.7569 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.87it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/40] Train: loss=4.2388 acc_cls=0.9990 acc_txt=1.0000 | Val:   loss=4.5378 acc_cls=0.8725 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/40] Train: loss=3.9741 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=4.3296 acc_cls=0.8912 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.91it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/40] Train: loss=3.7134 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=4.1273 acc_cls=0.8971 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.91it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/40] Train: loss=3.4597 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.9306 acc_cls=0.9088 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.90it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/40] Train: loss=3.2121 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.7392 acc_cls=0.9078 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.89it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/40] Train: loss=2.9716 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.5539 acc_cls=0.9108 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/40] Train: loss=2.7391 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.3751 acc_cls=0.9118 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 18.01it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/40] Train: loss=2.5150 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.2034 acc_cls=0.9118 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.94it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/40] Train: loss=2.3016 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=3.0391 acc_cls=0.9137 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.89it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/40] Train: loss=2.0965 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.8826 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/40] Train: loss=1.9042 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.7344 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/40] Train: loss=1.7248 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.5951 acc_cls=0.9147 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.99it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/40] Train: loss=1.5577 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.4646 acc_cls=0.9137 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.91it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/40] Train: loss=1.4033 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.3435 acc_cls=0.9137 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/40] Train: loss=1.2636 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.2313 acc_cls=0.9167 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/40] Train: loss=1.1362 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.1279 acc_cls=0.9147 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/40] Train: loss=1.0218 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.0334 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 18.04it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/40] Train: loss=0.9194 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.9467 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.97it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/40] Train: loss=0.8296 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.8677 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.97it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/40] Train: loss=0.7490 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.7957 acc_cls=0.9176 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.89it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/40] Train: loss=0.6777 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.7297 acc_cls=0.9167 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/40] Train: loss=0.6151 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.6700 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/40] Train: loss=0.5597 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.6154 acc_cls=0.9157 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/40] Train: loss=0.5109 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.5658 acc_cls=0.9176 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.94it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/40] Train: loss=0.4675 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.5203 acc_cls=0.9176 acc_txt=0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.91it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/40] Train: loss=0.4293 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.4786 acc_cls=0.9167 acc_txt=0.7696\n",
      "Early stop triggered...Exiting on epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 193/193 [00:10<00:00, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=1.8457  acc_cls=0.9117  acc_txt=0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "1d0690dad522e224",
   "metadata": {},
   "source": [
    "## Save trained model\n",
    "### Save head layer and LoRA layers only"
   ]
  },
  {
   "cell_type": "code",
   "id": "80d54e14fbbd12cd",
   "metadata": {},
   "source": [
    "# Save Trained weights, as only head layer is trained\n",
    "def save_light_model(path,clip_model,head):\n",
    "    # sort out LoRA layers\n",
    "    lora_states = {k:v for k,v in clip_model.state_dict().items() if \"lora_\" in k}\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "        \"lora_states\": lora_states,\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Light weight model (only contains head and LoRA) saved to {path}\")\n",
    "\n",
    "\n",
    "    # Return clip_lora, head\n",
    "def load_light_simple(path, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    missing, unexpected = clip_model.load_state_dict(checkpoint[\"lora_states\"], strict=False) # set to false allow partial loading\n",
    "    print(missing, unexpected)\n",
    "\n",
    "    feat_dim = clip_model.config.projection_dim\n",
    "    head = nn.Linear(feat_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"[light] loaded ← {path}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Style Saving\n",
    "\n",
    "def save_full_dir(output_dir, clip_model, head):\n",
    "    output = Path(output_dir)\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    clip_model.save_pretrained(output)\n",
    "    torch.save({\"num_classes\": head.out_features,\n",
    "                \"state_dict\": head.state_dict()}, output/\"head.pt\")\n",
    "    print(f\"[full-dir] saved → {output}\")\n",
    "\n",
    "def load_full_dir(output_dir, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    from transformers import CLIPModel\n",
    "    output = Path(output_dir)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(output).to(device)  # Load from folder\n",
    "\n",
    "    head_ckpt = torch.load(output/\"head.pt\", map_location=device)\n",
    "    import torch.nn as nn\n",
    "    head = nn.Linear(clip_model.config.projection_dim, head_ckpt[\"num_classes\"]).to(device)\n",
    "    head.load_state_dict(head_ckpt[\"state_dict\"])\n",
    "    print(f\"[full-dir] loaded ← {output}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Single File\n",
    "def save_full_model(path,clip_model,head):\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"clip_state_dict\": clip_model.state_dict(),\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Full  model  saved to {path}\")\n",
    "\n",
    "def load_full_model(path,device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    print(checkpoint[\"clip_name\"])\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    clip_model.load_state_dict(checkpoint[\"clip_state_dict\"], strict=True) # This time true cause loading full model\n",
    "\n",
    "    head = nn.Linear(clip_model.config.projection_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"Full  model  loaded from {path}\")\n",
    "    return clip_model, head\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b46319ea6fa5074",
   "metadata": {},
   "source": [
    "# Actual Saving Code\n",
    "os.makedirs(\"model\",exist_ok=True)\n",
    "save_light_model(\"model/clip_weights.pt\", clip_model, head)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62f1c97dc88c2105",
   "metadata": {},
   "source": [
    "# Try loading one\n",
    "clip_model, head = load_light_simple(\"model/clip_weights.pt\")\n",
    "\n",
    "from itertools import islice\n",
    "clip_model.eval()\n",
    "head.eval()\n",
    "processor = CLIPProcessor.from_pretrained(getattr(clip_model, \"name_or_path\", \"openai/clip-vit-base-patch32\"))\n",
    "\n",
    "def collate_pil(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "val_set  = datasets.Flowers102(root=\"../data\", split=\"val\",  download=True)\n",
    "test_set = datasets.Flowers102(root=\"../data\", split=\"test\", download=True)\n",
    "val_loader  = DataLoader(val_set,  batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_first(loader):\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        feats = clip_model.get_image_features(**inputs)          # [B, D]\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        logits = head(feats)                                     # [B, C]\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc  = evaluate_first(val_loader)\n",
    "test_acc = evaluate_first(test_loader)\n",
    "print(f\"Val Acc = {val_acc:.4f} | Test Acc = {test_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eabd4d4d884e5851",
   "metadata": {},
   "source": [
    "# This block saves full model\n",
    "\n",
    "# save_full_model(\"model/full_model.pt\", clip_model, head)\n",
    "# clip_model, head = load_full_model(\"model/full_model.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "273ac7ac8f4d5287"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
