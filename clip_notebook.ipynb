{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:36.840434Z",
     "start_time": "2025-10-28T08:46:36.827705Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import pipeline, CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "LOADER_PATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Cuda Availability:{torch.cuda.is_available()} Training on {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Availability:True Training on cuda\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "c24885366b00e5e9",
   "metadata": {},
   "source": [
    "# Notebook for interactive testing for CLIP"
   ]
  },
  {
   "cell_type": "code",
   "id": "afdd8de3917c931a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:42.065981Z",
     "start_time": "2025-10-28T08:46:37.637335Z"
    }
   },
   "source": [
    "\n",
    "class Cfg:\n",
    "    model_id: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    seed:   int = 42\n",
    "\n",
    "    # -------- Optim & Loss ----------\n",
    "    lr_head: float = 1e-3      # 线性头\n",
    "    wd_head: float = 1e-4\n",
    "    lr_lora: float = 1e-4      # LoRA 注入层\n",
    "    wd_lora: float = 1e-2\n",
    "    lambda_text: float = 0.3   # 文本对齐辅助损失权重\n",
    "    \n",
    "    early_stopping: bool = True\n",
    "    early_stop_patience = 3\n",
    "    early_stop_minimum_improvement:float = 0.02\n",
    "    \n",
    "\n",
    "    # -------- LoRA ----------\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_target: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")  # 只对注意力投影层做LoRA\n",
    "    # 也可扩展到 MLP 内部 proj，但注意稳定性\n",
    "\n",
    "    amp: bool = True\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "PREPROCESS_DATA_ROOT = \"data/preprocessed\"\n",
    "torch.manual_seed(cfg.seed)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device).eval()\n",
    "processor  = CLIPProcessor.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "2a9dfdb42fcf2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:43.038777Z",
     "start_time": "2025-10-28T08:46:42.066988Z"
    }
   },
   "source": [
    "# ---------- 2. Load Dataset ----------\n",
    "def collate_pil(batch):\n",
    "    # batch: List[ (PIL.Image.Image, int) ]\n",
    "    images, labels = zip(*batch)           # images: tuple of PIL, labels: tuple of int\n",
    "    return list(images), torch.tensor(labels)  # 让 processor 接收 list[PIL]，labels 变成 LongTensor\n",
    "\n",
    "def preprocess_dataset(split=\"train\",data_root='data',outputdir=\"data/preprocessed\",batchsize=cfg.batch_size,fp16=False):\n",
    "    os.makedirs(outputdir,exist_ok=True)\n",
    "    dataset = datasets.Flowers102(root=data_root, split=split, download=True)\n",
    "    classes = dataset.classes\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle= True if split==\"train\" else False, num_workers=0,collate_fn=collate_pil)\n",
    "    \n",
    "    # Allocate RAM\n",
    "    N = len(dataset)\n",
    "    C, H, W = 3, 224, 224\n",
    "    pixels = torch.empty((N,C,H,W), dtype=torch.float32 if not fp16 else torch.float16)\n",
    "    labels = torch.empty(N, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Preprocessing {split} data...\")\n",
    "    \n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for images, y in tqdm(loader,desc=f\"Preprocessing {split}\"):\n",
    "            pix = processor(images=images,return_tensors=\"pt\")['pixel_values']\n",
    "            if fp16:\n",
    "                pix = pix.half()\n",
    "            b = pix.size(0) # patch size\n",
    "            pixels[index:index+b] = pix\n",
    "            labels[index:index+b] = y\n",
    "            index += b\n",
    "    \n",
    "    \n",
    "    out_path = os.path.join(outputdir, f\"{split}.pt\")\n",
    "    torch.save({\"pixel_values\": pixels, \"labels\": labels, \"fp16\": fp16,\"classes\":classes}, out_path)\n",
    "    print(f\"Saved → {out_path} (pixels: {pixels.shape}, dtype={pixels.dtype})\")\n",
    "    \n",
    "    print(\"Performing Garbage Cleaning...\")\n",
    "    del pixels, labels\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # for notebook\n",
    "    %reset_selective -f pixels\n",
    "    %reset_selective -f labels\n",
    "\n",
    "class CacheDataset:\n",
    "    def __init__(self, split=\"train\", root=\"data/preprocessed\"):\n",
    "        path = os.path.join(root, f\"{split}.pt\")\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "        self.pixel_values = obj[\"pixel_values\"]\n",
    "        self.labels = obj[\"labels\"]\n",
    "        self.fp16 = bool(obj.get(\"fp16\", False))\n",
    "        self.classes = obj['classes']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels.numel()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pixel_values[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "# Preprocess and save dataset                \n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"train.pt\")):    \n",
    "    preprocess_dataset(split=\"train\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"val.pt\")):  \n",
    "    preprocess_dataset(split=\"val\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"test.pt\")):  \n",
    "    preprocess_dataset(split=\"test\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Load dataset\n",
    "train_set = CacheDataset(split=\"train\")\n",
    "val_set = CacheDataset(split=\"val\")\n",
    "test_set = CacheDataset(split=\"test\")\n",
    "\n",
    "classname = val_set.classes\n",
    "classname\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink primrose',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'canterbury bells',\n",
       " 'sweet pea',\n",
       " 'english marigold',\n",
       " 'tiger lily',\n",
       " 'moon orchid',\n",
       " 'bird of paradise',\n",
       " 'monkshood',\n",
       " 'globe thistle',\n",
       " 'snapdragon',\n",
       " \"colt's foot\",\n",
       " 'king protea',\n",
       " 'spear thistle',\n",
       " 'yellow iris',\n",
       " 'globe-flower',\n",
       " 'purple coneflower',\n",
       " 'peruvian lily',\n",
       " 'balloon flower',\n",
       " 'giant white arum lily',\n",
       " 'fire lily',\n",
       " 'pincushion flower',\n",
       " 'fritillary',\n",
       " 'red ginger',\n",
       " 'grape hyacinth',\n",
       " 'corn poppy',\n",
       " 'prince of wales feathers',\n",
       " 'stemless gentian',\n",
       " 'artichoke',\n",
       " 'sweet william',\n",
       " 'carnation',\n",
       " 'garden phlox',\n",
       " 'love in the mist',\n",
       " 'mexican aster',\n",
       " 'alpine sea holly',\n",
       " 'ruby-lipped cattleya',\n",
       " 'cape flower',\n",
       " 'great masterwort',\n",
       " 'siam tulip',\n",
       " 'lenten rose',\n",
       " 'barbeton daisy',\n",
       " 'daffodil',\n",
       " 'sword lily',\n",
       " 'poinsettia',\n",
       " 'bolero deep blue',\n",
       " 'wallflower',\n",
       " 'marigold',\n",
       " 'buttercup',\n",
       " 'oxeye daisy',\n",
       " 'common dandelion',\n",
       " 'petunia',\n",
       " 'wild pansy',\n",
       " 'primula',\n",
       " 'sunflower',\n",
       " 'pelargonium',\n",
       " 'bishop of llandaff',\n",
       " 'gaura',\n",
       " 'geranium',\n",
       " 'orange dahlia',\n",
       " 'pink-yellow dahlia?',\n",
       " 'cautleya spicata',\n",
       " 'japanese anemone',\n",
       " 'black-eyed susan',\n",
       " 'silverbush',\n",
       " 'californian poppy',\n",
       " 'osteospermum',\n",
       " 'spring crocus',\n",
       " 'bearded iris',\n",
       " 'windflower',\n",
       " 'tree poppy',\n",
       " 'gazania',\n",
       " 'azalea',\n",
       " 'water lily',\n",
       " 'rose',\n",
       " 'thorn apple',\n",
       " 'morning glory',\n",
       " 'passion flower',\n",
       " 'lotus',\n",
       " 'toad lily',\n",
       " 'anthurium',\n",
       " 'frangipani',\n",
       " 'clematis',\n",
       " 'hibiscus',\n",
       " 'columbine',\n",
       " 'desert-rose',\n",
       " 'tree mallow',\n",
       " 'magnolia',\n",
       " 'cyclamen',\n",
       " 'watercress',\n",
       " 'canna lily',\n",
       " 'hippeastrum',\n",
       " 'bee balm',\n",
       " 'ball moss',\n",
       " 'foxglove',\n",
       " 'bougainvillea',\n",
       " 'camellia',\n",
       " 'mallow',\n",
       " 'mexican petunia',\n",
       " 'bromelia',\n",
       " 'blanket flower',\n",
       " 'trumpet creeper',\n",
       " 'blackberry lily']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:43.041072Z",
     "start_time": "2025-10-28T08:46:43.038777Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1a8294a5bd088ae",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "e55416c71fb1fdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:43.069590Z",
     "start_time": "2025-10-28T08:46:43.041072Z"
    }
   },
   "source": [
    "# ---------- 3. DataLoader ----------\n",
    "# ---------- Process of images is put on epoch loops\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True\n",
    "    #workers should be 4, but got problems in notebook\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "ea3ab3c785fcbdf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Setting and Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d9682cb89ba6dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T08:46:44.088769Z",
     "start_time": "2025-10-28T08:46:43.069590Z"
    }
   },
   "source": [
    "# Word Prompt Embedding\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "promptTemplate = {\n",
    "    \"A photo of {}.\",\n",
    "    \"A photo of flower {}.\",\n",
    "    \"Botanic picture of {}\",\n",
    "    \"A example picture of type {}\"\n",
    "}\n",
    "# Use more templates to reduce sensitivity to other contexts\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(names):\n",
    "    embs = []\n",
    "    for name in tqdm(names, desc=\"TextEmbed\"):\n",
    "        prompts = [t.format(name.replace(\"_\",\" \")) for t in promptTemplate] # insert class names\n",
    "        inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        te = clip_model.get_text_features(**inputs)     # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        embs.append(te.mean(dim=0))                     # [D]\n",
    "    text = torch.stack(embs, dim=0)                     # [C, D]\n",
    "    return text / text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embs = build_text_embeddings(classname)          # [102, D], 固定不训练\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextEmbed: 100%|██████████| 102/102 [00:01<00:00, 100.74it/s]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "c5246531ba1768ef",
   "metadata": {},
   "source": [
    "----\n",
    "## Building CLIP model with LoRA and word embedding.\n",
    "\n",
    "Have to implement a LoRA linear layer ourselves."
   ]
  },
  {
   "cell_type": "code",
   "id": "63a9ba3e82edb850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:31:54.228616Z",
     "start_time": "2025-10-28T06:31:54.220619Z"
    }
   },
   "source": [
    "# LoRA Injection (Trains LoRA matries only)\n",
    "import torch, torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "\n",
    "# y = w0 + x*(BA)*alpha/rank\n",
    "# Shape of A: din by rank / Shape of B: rank by dout\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r=8, alpha=16, dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.base = base # linear layer frozen for training LoRA parameters\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "        dev = base.weight.device\n",
    "        dt  = base.weight.dtype\n",
    "\n",
    "        \n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(base.in_features, r, bias=False).to(dev, dtype=dt)\n",
    "            self.lora_B = nn.Linear(r, base.out_features, bias=False).to(dev, dtype=dt)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight,a=5**0.5)\n",
    "            nn.init.zeros_(self.lora_B.weight) # set B to 0, avoid any bias introduced.\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "            #Frozen\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return self.base(x)\n",
    "\n",
    "\n",
    "# LoRA Injection with warped LoRA layer shown above.\n",
    "\n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "    clip_model.to(device)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "8a444b54cc7e7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:31:58.564938Z",
     "start_time": "2025-10-28T06:31:58.554578Z"
    }
   },
   "source": [
    "head, optimizer, scaler = build_head_and_optim(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_image_feats(images):\n",
    "    # images are actual tensor now\n",
    "    # assume it's preprocessed\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        pixel_values = images.to(device, dtype=torch.float16 if (device==\"cuda\" and cfg.amp) else torch.float32)\n",
    "    else:\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        \n",
    "    feats = clip_model.get_image_features(pixel_values=pixel_values)           # [B, D]\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ecf8ad5eafb9cbda",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Main Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "id": "75d9230be58c43ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:32:55.319113Z",
     "start_time": "2025-10-28T06:32:55.313182Z"
    }
   },
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.last_loss = 0\n",
    "        self.patience = cfg.early_stop_patience\n",
    "        self.enable = cfg.early_stopping\n",
    "        self.delta = cfg.early_stop_minimum_improvement\n",
    "    def report(self,loss):\n",
    "        if self.last_loss - loss < self.delta:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        self.last_loss = loss\n",
    "    \n",
    "    def stop_flag(self):\n",
    "        return self.enable and (self.counter >= self.patience)\n",
    "        \n",
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = get_image_feats(images)                   # [B, D]\n",
    "\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "        \n",
    "    loss_avg = loss_sum / total\n",
    "    \n",
    "    return {\n",
    "    \"loss\": loss_avg,\n",
    "    \"acc_cls\": correct_cls/total,   # 线性头准确率\n",
    "    \"acc_txt\": correct_txt/total,   # 文本读出准确率（zero-shot 风格）\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "11a145942664fd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:35:27.954872Z",
     "start_time": "2025-10-28T06:32:55.729490Z"
    }
   },
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "earlystop = EarlyStopper()\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"[{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "    earlystop.report(val['loss'])\n",
    "    if earlystop.stop_flag():\n",
    "        print(f\"Early stop triggered...Exiting on epoch {ep}\")\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 16.37it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/40] Train: loss=4.8075 acc_cls=0.9029 acc_txt=0.6392 | Val:   loss=4.7781 acc_cls=0.8333 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.79it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/40] Train: loss=4.7370 acc_cls=0.9216 acc_txt=0.6392 | Val:   loss=4.7145 acc_cls=0.8441 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.58it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/40] Train: loss=4.6672 acc_cls=0.9324 acc_txt=0.6392 | Val:   loss=4.6511 acc_cls=0.8520 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.86it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/40] Train: loss=4.5991 acc_cls=0.9333 acc_txt=0.6392 | Val:   loss=4.5890 acc_cls=0.8608 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.90it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/40] Train: loss=4.5300 acc_cls=0.9471 acc_txt=0.6392 | Val:   loss=4.5275 acc_cls=0.8745 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/40] Train: loss=4.4637 acc_cls=0.9500 acc_txt=0.6392 | Val:   loss=4.4665 acc_cls=0.8647 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.90it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/40] Train: loss=4.3966 acc_cls=0.9520 acc_txt=0.6392 | Val:   loss=4.4063 acc_cls=0.8775 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/40] Train: loss=4.3302 acc_cls=0.9520 acc_txt=0.6392 | Val:   loss=4.3467 acc_cls=0.8676 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.89it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/40] Train: loss=4.2649 acc_cls=0.9578 acc_txt=0.6392 | Val:   loss=4.2880 acc_cls=0.8706 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/40] Train: loss=4.2000 acc_cls=0.9618 acc_txt=0.6392 | Val:   loss=4.2299 acc_cls=0.8794 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/40] Train: loss=4.1364 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=4.1720 acc_cls=0.8765 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/40] Train: loss=4.0726 acc_cls=0.9627 acc_txt=0.6392 | Val:   loss=4.1154 acc_cls=0.8814 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.94it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/40] Train: loss=4.0109 acc_cls=0.9627 acc_txt=0.6392 | Val:   loss=4.0593 acc_cls=0.8824 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/40] Train: loss=3.9493 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=4.0040 acc_cls=0.8804 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.96it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/40] Train: loss=3.8876 acc_cls=0.9618 acc_txt=0.6392 | Val:   loss=3.9491 acc_cls=0.8814 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/40] Train: loss=3.8271 acc_cls=0.9598 acc_txt=0.6392 | Val:   loss=3.8952 acc_cls=0.8814 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.91it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/40] Train: loss=3.7675 acc_cls=0.9627 acc_txt=0.6392 | Val:   loss=3.8418 acc_cls=0.8794 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.96it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/40] Train: loss=3.7093 acc_cls=0.9618 acc_txt=0.6392 | Val:   loss=3.7892 acc_cls=0.8794 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/40] Train: loss=3.6510 acc_cls=0.9627 acc_txt=0.6392 | Val:   loss=3.7374 acc_cls=0.8824 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.94it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/40] Train: loss=3.5936 acc_cls=0.9637 acc_txt=0.6392 | Val:   loss=3.6861 acc_cls=0.8804 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/40] Train: loss=3.5373 acc_cls=0.9637 acc_txt=0.6392 | Val:   loss=3.6360 acc_cls=0.8843 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.96it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/40] Train: loss=3.4815 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=3.5859 acc_cls=0.8804 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/40] Train: loss=3.4257 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=3.5370 acc_cls=0.8824 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/40] Train: loss=3.3718 acc_cls=0.9647 acc_txt=0.6392 | Val:   loss=3.4885 acc_cls=0.8824 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.86it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/40] Train: loss=3.3191 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=3.4410 acc_cls=0.8804 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.88it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/40] Train: loss=3.2661 acc_cls=0.9647 acc_txt=0.6392 | Val:   loss=3.3943 acc_cls=0.8843 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.96it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/40] Train: loss=3.2144 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=3.3481 acc_cls=0.8863 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.99it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/40] Train: loss=3.1636 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=3.3027 acc_cls=0.8853 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.97it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/40] Train: loss=3.1127 acc_cls=0.9647 acc_txt=0.6392 | Val:   loss=3.2580 acc_cls=0.8853 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.99it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/40] Train: loss=3.0644 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=3.2141 acc_cls=0.8853 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 18.00it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/40] Train: loss=3.0160 acc_cls=0.9627 acc_txt=0.6392 | Val:   loss=3.1710 acc_cls=0.8814 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/40] Train: loss=2.9675 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=3.1285 acc_cls=0.8882 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.92it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/40] Train: loss=2.9203 acc_cls=0.9647 acc_txt=0.6392 | Val:   loss=3.0867 acc_cls=0.8853 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/40] Train: loss=2.8739 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=3.0457 acc_cls=0.8853 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.98it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/40] Train: loss=2.8289 acc_cls=0.9676 acc_txt=0.6392 | Val:   loss=3.0054 acc_cls=0.8922 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.99it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/40] Train: loss=2.7846 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=2.9657 acc_cls=0.8863 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 18.00it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/40] Train: loss=2.7408 acc_cls=0.9667 acc_txt=0.6392 | Val:   loss=2.9267 acc_cls=0.8863 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/40] Train: loss=2.6983 acc_cls=0.9696 acc_txt=0.6392 | Val:   loss=2.8887 acc_cls=0.8912 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.95it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/40] Train: loss=2.6565 acc_cls=0.9696 acc_txt=0.6392 | Val:   loss=2.8513 acc_cls=0.8941 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:01<00:00, 17.99it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/40] Train: loss=2.6149 acc_cls=0.9657 acc_txt=0.6392 | Val:   loss=2.8145 acc_cls=0.8892 acc_txt=0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 193/193 [00:10<00:00, 18.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=2.8892  acc_cls=0.8922  acc_txt=0.6451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "1d0690dad522e224",
   "metadata": {},
   "source": [
    "## Save trained model\n",
    "### Save head layer and LoRA layers only"
   ]
  },
  {
   "cell_type": "code",
   "id": "80d54e14fbbd12cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:37:32.109619Z",
     "start_time": "2025-10-28T06:37:32.102618Z"
    }
   },
   "source": [
    "# Save Trained weights, as only head layer is trained\n",
    "def save_light_model(path,clip_model,head):\n",
    "    # sort out LoRA layers\n",
    "    lora_states = {k:v for k,v in clip_model.state_dict().items() if \"lora_\" in k}\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "        \"lora_states\": lora_states,\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Light weight model (only contains head and LoRA) saved to {path}\")\n",
    "\n",
    "\n",
    "    # Return clip_lora, head\n",
    "def load_light_simple(path, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    missing, unexpected = clip_model.load_state_dict(checkpoint[\"lora_states\"], strict=False) # set to false allow partial loading\n",
    "    print(missing, unexpected)\n",
    "\n",
    "    feat_dim = clip_model.config.projection_dim\n",
    "    head = nn.Linear(feat_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"[light] loaded ← {path}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Style Saving\n",
    "\n",
    "def save_full_dir(output_dir, clip_model, head):\n",
    "    output = Path(output_dir)\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    clip_model.save_pretrained(output)           # 保存到目录（含 LoRA 参数）\n",
    "    torch.save({\"num_classes\": head.out_features,\n",
    "                \"state_dict\": head.state_dict()}, output/\"head.pt\")\n",
    "    print(f\"[full-dir] saved → {output}\")\n",
    "\n",
    "def load_full_dir(output_dir, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    from transformers import CLIPModel\n",
    "    output = Path(output_dir)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(output).to(device)  # Load from folder\n",
    "    # 如你的保存目录包含了 LoRA 权重（因为它在 state_dict 里），这一步就不用再注入；\n",
    "    # 如果恢复后发现没有 LoRA 结构，可与上面相同：先注入再 load。\n",
    "\n",
    "    head_ckpt = torch.load(output/\"head.pt\", map_location=device)\n",
    "    import torch.nn as nn\n",
    "    head = nn.Linear(clip_model.config.projection_dim, head_ckpt[\"num_classes\"]).to(device)\n",
    "    head.load_state_dict(head_ckpt[\"state_dict\"])\n",
    "    print(f\"[full-dir] loaded ← {output}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Single File\n",
    "def save_full_model(path,clip_model,head):\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"clip_state_dict\": clip_model.state_dict(),\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Full  model  saved to {path}\")\n",
    "\n",
    "def load_full_model(path,device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    print(checkpoint[\"clip_name\"])\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    clip_model.load_state_dict(checkpoint[\"clip_state_dict\"], strict=True) # This time true cause loading full model\n",
    "\n",
    "    head = nn.Linear(clip_model.config.projection_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"Full  model  loaded from {path}\")\n",
    "    return clip_model, head\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7b46319ea6fa5074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:37:33.569898Z",
     "start_time": "2025-10-28T06:37:33.551942Z"
    }
   },
   "source": [
    "# Actual Saving Code\n",
    "os.makedirs(\"model\",exist_ok=True)\n",
    "save_light_model(\"model/clip_weights.pt\", clip_model, head)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light weight model (only contains head and LoRA) saved to model/clip_weights.pt\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "62f1c97dc88c2105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T06:38:45.278740Z",
     "start_time": "2025-10-28T06:37:34.655336Z"
    }
   },
   "source": [
    "# Try loading one\n",
    "clip_model, head = load_light_simple(\"model/clip_weights.pt\")\n",
    "\n",
    "from itertools import islice\n",
    "clip_model.eval()\n",
    "head.eval()\n",
    "processor = CLIPProcessor.from_pretrained(getattr(clip_model, \"name_or_path\", \"openai/clip-vit-base-patch32\"))\n",
    "\n",
    "def collate_pil(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "val_set  = datasets.Flowers102(root=\"./data\", split=\"val\",  download=True)\n",
    "test_set = datasets.Flowers102(root=\"./data\", split=\"test\", download=True)\n",
    "val_loader  = DataLoader(val_set,  batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_first(loader):\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        feats = clip_model.get_image_features(**inputs)          # [B, D]\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        logits = head(feats)                                     # [B, C]\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc  = evaluate_first(val_loader)\n",
    "test_acc = evaluate_first(test_loader)\n",
    "print(f\"Val Acc = {val_acc:.4f} | Test Acc = {test_acc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['logit_scale', 'text_model.embeddings.token_embedding.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.final_layer_norm.weight', 'text_model.final_layer_norm.bias', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.post_layernorm.bias', 'visual_projection.weight', 'text_projection.weight'] []\n",
      "[light] loaded ← model/clip_weights.pt\n",
      "Val Acc = 0.8941 | Test Acc = 0.8920\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eabd4d4d884e5851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T16:32:24.305874Z",
     "start_time": "2025-10-27T16:32:20.307411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full  model  saved to model/full_model.pt\n"
     ]
    }
   ],
   "source": [
    "# This block saves full model\n",
    "\n",
    "# save_full_model(\"model/full_model.pt\", clip_model, head)\n",
    "# clip_model, head = load_full_model(\"model/full_model.pt\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Fine-Tuning of CLIP: Visual Prompt Tuning\n",
    "\n",
    "We insert a learnable prompt before encoder of transformer, along with the images."
   ],
   "id": "d378522a8563fc31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Config\n",
    "VISUAL_PROMPT_SIZE = 8\n",
    "\n",
    "class VPTShallowClip(nn.Module):\n",
    "    def __init__(self, clip_model: CLIPModel, init_std: float = 0.02, freeze_backbone: bool = True):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        vm = self.clip.vision_model\n",
    "        D = vm.config.hidden_size # Hidden size of embedding\n",
    "        m = VISUAL_PROMPT_SIZE\n",
    "        self.prompt =   nn.Parameter(torch.randn(m, D) * init_std)\n",
    "        self.prompt_pos = nn.Parameter(torch.zeros(1,m, D)) # Positional Encoding, each one is different thus need B different ones.\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for p in self.clip.vision_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.clip.visual_projection.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def embed_with_pos(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # reuse\n",
    "        return self.clip.vision_model.embeddings(pixel_values)\n",
    "    \n",
    "    def _encode_with_prompts(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # CLS+ img patch = [B, 1+N, D]\n",
    "    # we add prompt and prompt_pos into the vector before the actual image.\n",
    "    \n",
    "        vm = self.clip.vision_model\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, -1)\n",
    "        cls, patches = x[:,0, :], x[:,1:,:]\n",
    "        p = self.prompt + self.prompt_pos # positional embedding\n",
    "        p = expand"
   ],
   "id": "eef4e448fdbd5fb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
