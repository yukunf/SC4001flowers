{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:01.783617Z",
     "start_time": "2025-10-27T14:46:59.035354Z"
    }
   },
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import pipeline, CLIPModel, CLIPProcessor\n",
    "\n",
    "# Configuration\n",
    "LOADER_PATCH_SIZE = 32\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Notebook for interactive testing for CLIP",
   "id": "c24885366b00e5e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:05.452991Z",
     "start_time": "2025-10-27T14:47:01.843876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Cfg:\n",
    "    model_id: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 20\n",
    "    seed:   int = 42\n",
    "\n",
    "    # -------- Optim & Loss ----------\n",
    "    lr_head: float = 1e-3      # 线性头\n",
    "    wd_head: float = 1e-4\n",
    "    lr_lora: float = 1e-4      # LoRA 注入层\n",
    "    wd_lora: float = 1e-2\n",
    "    lambda_text: float = 0.3   # 文本对齐辅助损失权重（链路3）\n",
    "\n",
    "    # -------- LoRA ----------\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_target: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")  # 只对注意力投影层做LoRA\n",
    "    # 也可扩展到 MLP 内部 proj，但注意稳定性\n",
    "\n",
    "    amp: bool = True\n",
    "\n",
    "cfg = Cfg()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(cfg.seed)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device).eval()\n",
    "processor  = CLIPProcessor.from_pretrained(model_id)"
   ],
   "id": "afdd8de3917c931a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:29.887550Z",
     "start_time": "2025-10-27T14:47:29.862077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 2. Load Dataset ----------\n",
    "# We do transform in Epoch loops\n",
    "train_set = datasets.Flowers102(root=\"./data\", split=\"train\", download=True)\n",
    "val_set   = datasets.Flowers102(root=\"./data\", split=\"val\",   download=True)\n",
    "test_set  = datasets.Flowers102(root=\"./data\", split=\"test\",  download=True)\n",
    "classname = val_set.classes\n",
    "classname"
   ],
   "id": "2a9dfdb42fcf2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink primrose',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'canterbury bells',\n",
       " 'sweet pea',\n",
       " 'english marigold',\n",
       " 'tiger lily',\n",
       " 'moon orchid',\n",
       " 'bird of paradise',\n",
       " 'monkshood',\n",
       " 'globe thistle',\n",
       " 'snapdragon',\n",
       " \"colt's foot\",\n",
       " 'king protea',\n",
       " 'spear thistle',\n",
       " 'yellow iris',\n",
       " 'globe-flower',\n",
       " 'purple coneflower',\n",
       " 'peruvian lily',\n",
       " 'balloon flower',\n",
       " 'giant white arum lily',\n",
       " 'fire lily',\n",
       " 'pincushion flower',\n",
       " 'fritillary',\n",
       " 'red ginger',\n",
       " 'grape hyacinth',\n",
       " 'corn poppy',\n",
       " 'prince of wales feathers',\n",
       " 'stemless gentian',\n",
       " 'artichoke',\n",
       " 'sweet william',\n",
       " 'carnation',\n",
       " 'garden phlox',\n",
       " 'love in the mist',\n",
       " 'mexican aster',\n",
       " 'alpine sea holly',\n",
       " 'ruby-lipped cattleya',\n",
       " 'cape flower',\n",
       " 'great masterwort',\n",
       " 'siam tulip',\n",
       " 'lenten rose',\n",
       " 'barbeton daisy',\n",
       " 'daffodil',\n",
       " 'sword lily',\n",
       " 'poinsettia',\n",
       " 'bolero deep blue',\n",
       " 'wallflower',\n",
       " 'marigold',\n",
       " 'buttercup',\n",
       " 'oxeye daisy',\n",
       " 'common dandelion',\n",
       " 'petunia',\n",
       " 'wild pansy',\n",
       " 'primula',\n",
       " 'sunflower',\n",
       " 'pelargonium',\n",
       " 'bishop of llandaff',\n",
       " 'gaura',\n",
       " 'geranium',\n",
       " 'orange dahlia',\n",
       " 'pink-yellow dahlia?',\n",
       " 'cautleya spicata',\n",
       " 'japanese anemone',\n",
       " 'black-eyed susan',\n",
       " 'silverbush',\n",
       " 'californian poppy',\n",
       " 'osteospermum',\n",
       " 'spring crocus',\n",
       " 'bearded iris',\n",
       " 'windflower',\n",
       " 'tree poppy',\n",
       " 'gazania',\n",
       " 'azalea',\n",
       " 'water lily',\n",
       " 'rose',\n",
       " 'thorn apple',\n",
       " 'morning glory',\n",
       " 'passion flower',\n",
       " 'lotus',\n",
       " 'toad lily',\n",
       " 'anthurium',\n",
       " 'frangipani',\n",
       " 'clematis',\n",
       " 'hibiscus',\n",
       " 'columbine',\n",
       " 'desert-rose',\n",
       " 'tree mallow',\n",
       " 'magnolia',\n",
       " 'cyclamen',\n",
       " 'watercress',\n",
       " 'canna lily',\n",
       " 'hippeastrum',\n",
       " 'bee balm',\n",
       " 'ball moss',\n",
       " 'foxglove',\n",
       " 'bougainvillea',\n",
       " 'camellia',\n",
       " 'mallow',\n",
       " 'mexican petunia',\n",
       " 'bromelia',\n",
       " 'blanket flower',\n",
       " 'trumpet creeper',\n",
       " 'blackberry lily']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:32.314606Z",
     "start_time": "2025-10-27T14:47:32.310079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 3. DataLoader ----------\n",
    "# ---------- Process of images is put on epoch loops\n",
    "def collate_pil(batch):\n",
    "    # batch: List[ (PIL.Image.Image, int) ]\n",
    "    images, labels = zip(*batch)           # images: tuple of PIL, labels: tuple of int\n",
    "    return list(images), torch.tensor(labels)  # 让 processor 接收 list[PIL]，labels 变成 LongTensor\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    "    #workers should be 4, but got problems in notebook\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    ")"
   ],
   "id": "e55416c71fb1fdb0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Model Setting and Training"
   ],
   "id": "ea3ab3c785fcbdf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:37.449126Z",
     "start_time": "2025-10-27T14:47:35.666324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word Prompt Embedding\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "promptTemplate = {\n",
    "    \"A photo of {}.\",\n",
    "    \"A photo of flower {}.\",\n",
    "    \"Botanic picture of {}\",\n",
    "    \"A example picture of type {}\"\n",
    "}\n",
    "# Use more templates to reduce sensitivity to other contexts\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(names):\n",
    "    embs = []\n",
    "    for name in tqdm(names, desc=\"TextEmbed\"):\n",
    "        prompts = [t.format(name.replace(\"_\",\" \")) for t in promptTemplate] # insert class names\n",
    "        inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        te = clip_model.get_text_features(**inputs)     # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        embs.append(te.mean(dim=0))                     # [D]\n",
    "    text = torch.stack(embs, dim=0)                     # [C, D]\n",
    "    return text / text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embs = build_text_embeddings(classname)          # [102, D], 固定不训练\n",
    "\n"
   ],
   "id": "2d9682cb89ba6dc0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextEmbed: 100%|██████████| 102/102 [00:01<00:00, 57.74it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Building CLIP model with LoRA and word embedding.\n",
    "\n",
    "Have to implement a LoRA linear layer ourselves."
   ],
   "id": "c5246531ba1768ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:38.922886Z",
     "start_time": "2025-10-27T14:47:38.914989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LoRA Injection (Trains LoRA matries only)\n",
    "import torch, torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "\n",
    "# y = w0 + x*(BA)*alpha/rank\n",
    "# Shape of A: din by rank / Shape of B: rank by dout\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r=8, alpha=16, dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.base = base # linear layer frozen for training LoRA parameters\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(base.in_features, r, bias=False)\n",
    "            self.lora_B = nn.Linear(r, base.out_features, bias=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight,a=5**0.5)\n",
    "            nn.init.zeros_(self.lora_B.weight) # set B to 0, avoid any bias introduced.\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "            #Frozen\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return self.base(x)\n",
    "\n",
    "\n",
    "# LoRA Injection with warped LoRA layer shown above.\n",
    "\n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler\n"
   ],
   "id": "63a9ba3e82edb850",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:47:41.046769Z",
     "start_time": "2025-10-27T14:47:41.031331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "head, optimizer, scaler = build_head_and_optim(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_image_feats(images):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    feats = clip_model.get_image_features(**inputs)           # [B, D]\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()"
   ],
   "id": "8a444b54cc7e7aa4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Main Training Epoch"
   ],
   "id": "ecf8ad5eafb9cbda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T14:48:43.906023Z",
     "start_time": "2025-10-27T14:48:43.900462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = get_image_feats(images)                   # [B, D]\n",
    "\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "    return {\n",
    "    \"loss\": loss_sum/total,\n",
    "    \"acc_cls\": correct_cls/total,   # 线性头准确率\n",
    "    \"acc_txt\": correct_txt/total,   # 文本读出准确率（zero-shot 风格）\n",
    "}\n",
    "\n"
   ],
   "id": "75d9230be58c43ff",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:07:32.275167Z",
     "start_time": "2025-10-27T14:48:46.355120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"[{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")\n"
   ],
   "id": "11a145942664fd64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:34<00:00,  1.08s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20] Train: loss=5.0636 acc_cls=0.0529 acc_txt=0.6569 | Val:   loss=4.9526 acc_cls=0.2196 acc_txt=0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.00s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/20] Train: loss=4.8573 acc_cls=0.4549 acc_txt=0.7255 | Val:   loss=4.7771 acc_cls=0.6343 acc_txt=0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/20] Train: loss=4.6089 acc_cls=0.8108 acc_txt=0.8059 | Val:   loss=4.5698 acc_cls=0.8186 acc_txt=0.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/20] Train: loss=4.3251 acc_cls=0.9137 acc_txt=0.8833 | Val:   loss=4.3468 acc_cls=0.8696 acc_txt=0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/20] Train: loss=4.0390 acc_cls=0.9549 acc_txt=0.9480 | Val:   loss=4.1238 acc_cls=0.8873 acc_txt=0.8147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.01s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/20] Train: loss=3.7671 acc_cls=0.9676 acc_txt=0.9784 | Val:   loss=3.9013 acc_cls=0.8961 acc_txt=0.8088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/20] Train: loss=3.5170 acc_cls=0.9765 acc_txt=0.9873 | Val:   loss=3.6911 acc_cls=0.8971 acc_txt=0.8176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/20] Train: loss=3.2853 acc_cls=0.9833 acc_txt=0.9922 | Val:   loss=3.5087 acc_cls=0.9010 acc_txt=0.8088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/20] Train: loss=3.0651 acc_cls=0.9892 acc_txt=0.9980 | Val:   loss=3.3400 acc_cls=0.9059 acc_txt=0.8039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.00s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/20] Train: loss=2.8536 acc_cls=0.9912 acc_txt=0.9961 | Val:   loss=3.1697 acc_cls=0.9127 acc_txt=0.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.04s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/20] Train: loss=2.6537 acc_cls=0.9941 acc_txt=0.9971 | Val:   loss=3.0155 acc_cls=0.9059 acc_txt=0.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/20] Train: loss=2.4603 acc_cls=0.9971 acc_txt=0.9980 | Val:   loss=2.8564 acc_cls=0.9059 acc_txt=0.8010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/20] Train: loss=2.2702 acc_cls=0.9961 acc_txt=1.0000 | Val:   loss=2.7038 acc_cls=0.9078 acc_txt=0.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:18<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/20] Train: loss=2.0881 acc_cls=0.9971 acc_txt=1.0000 | Val:   loss=2.5601 acc_cls=0.9049 acc_txt=0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.02s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/20] Train: loss=1.9143 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.4343 acc_cls=0.9206 acc_txt=0.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/20] Train: loss=1.7480 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.3027 acc_cls=0.9127 acc_txt=0.8069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.05s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/20] Train: loss=1.5898 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.2090 acc_cls=0.9176 acc_txt=0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:31<00:00,  1.00it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/20] Train: loss=1.4403 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=2.0901 acc_cls=0.9167 acc_txt=0.7971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:33<00:00,  1.03s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/20] Train: loss=1.3014 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.9913 acc_cls=0.9206 acc_txt=0.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:34<00:00,  1.08s/it]\n",
      "Eval: 100%|██████████| 32/32 [00:17<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/20] Train: loss=1.1747 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.9094 acc_cls=0.9225 acc_txt=0.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 193/193 [01:49<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=1.9471  acc_cls=0.9167  acc_txt=0.7831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
