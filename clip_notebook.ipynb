{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T16:43:31.600734Z",
     "start_time": "2025-10-27T16:43:28.764925Z"
    }
   },
   "source": [
    "import os\n",
    "from itertools import islice\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import pipeline, CLIPModel, CLIPProcessor\n",
    "\n",
    "# Configuration\n",
    "LOADER_PATCH_SIZE = 32\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Notebook for interactive testing for CLIP",
   "id": "c24885366b00e5e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:54.401751Z",
     "start_time": "2025-10-27T15:55:50.872049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class Cfg:\n",
    "    model_id: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 20\n",
    "    seed:   int = 42\n",
    "\n",
    "    # -------- Optim & Loss ----------\n",
    "    lr_head: float = 1e-3      # 线性头\n",
    "    wd_head: float = 1e-4\n",
    "    lr_lora: float = 1e-4      # LoRA 注入层\n",
    "    wd_lora: float = 1e-2\n",
    "    lambda_text: float = 0.3   # 文本对齐辅助损失权重（链路3）\n",
    "\n",
    "    # -------- LoRA ----------\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_target: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")  # 只对注意力投影层做LoRA\n",
    "    # 也可扩展到 MLP 内部 proj，但注意稳定性\n",
    "\n",
    "    amp: bool = True\n",
    "\n",
    "cfg = Cfg()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(cfg.seed)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device).eval()\n",
    "processor  = CLIPProcessor.from_pretrained(model_id)"
   ],
   "id": "afdd8de3917c931a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:54.500430Z",
     "start_time": "2025-10-27T15:55:54.467748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 2. Load Dataset ----------\n",
    "\n",
    "train_set = datasets.Flowers102(root=\"./data\", split=\"train\", download=True)\n",
    "val_set   = datasets.Flowers102(root=\"./data\", split=\"val\",   download=True)\n",
    "test_set  = datasets.Flowers102(root=\"./data\", split=\"test\",  download=True)\n",
    "classname = val_set.classes\n",
    "classname\n",
    "\n",
    "'''\n",
    "ok, actually RAM is flooded if we load image first.\n",
    "So have to preprocess first. Just use the processor from CLIP\n",
    "'''\n",
    "\n"
   ],
   "id": "2a9dfdb42fcf2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink primrose',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'canterbury bells',\n",
       " 'sweet pea',\n",
       " 'english marigold',\n",
       " 'tiger lily',\n",
       " 'moon orchid',\n",
       " 'bird of paradise',\n",
       " 'monkshood',\n",
       " 'globe thistle',\n",
       " 'snapdragon',\n",
       " \"colt's foot\",\n",
       " 'king protea',\n",
       " 'spear thistle',\n",
       " 'yellow iris',\n",
       " 'globe-flower',\n",
       " 'purple coneflower',\n",
       " 'peruvian lily',\n",
       " 'balloon flower',\n",
       " 'giant white arum lily',\n",
       " 'fire lily',\n",
       " 'pincushion flower',\n",
       " 'fritillary',\n",
       " 'red ginger',\n",
       " 'grape hyacinth',\n",
       " 'corn poppy',\n",
       " 'prince of wales feathers',\n",
       " 'stemless gentian',\n",
       " 'artichoke',\n",
       " 'sweet william',\n",
       " 'carnation',\n",
       " 'garden phlox',\n",
       " 'love in the mist',\n",
       " 'mexican aster',\n",
       " 'alpine sea holly',\n",
       " 'ruby-lipped cattleya',\n",
       " 'cape flower',\n",
       " 'great masterwort',\n",
       " 'siam tulip',\n",
       " 'lenten rose',\n",
       " 'barbeton daisy',\n",
       " 'daffodil',\n",
       " 'sword lily',\n",
       " 'poinsettia',\n",
       " 'bolero deep blue',\n",
       " 'wallflower',\n",
       " 'marigold',\n",
       " 'buttercup',\n",
       " 'oxeye daisy',\n",
       " 'common dandelion',\n",
       " 'petunia',\n",
       " 'wild pansy',\n",
       " 'primula',\n",
       " 'sunflower',\n",
       " 'pelargonium',\n",
       " 'bishop of llandaff',\n",
       " 'gaura',\n",
       " 'geranium',\n",
       " 'orange dahlia',\n",
       " 'pink-yellow dahlia?',\n",
       " 'cautleya spicata',\n",
       " 'japanese anemone',\n",
       " 'black-eyed susan',\n",
       " 'silverbush',\n",
       " 'californian poppy',\n",
       " 'osteospermum',\n",
       " 'spring crocus',\n",
       " 'bearded iris',\n",
       " 'windflower',\n",
       " 'tree poppy',\n",
       " 'gazania',\n",
       " 'azalea',\n",
       " 'water lily',\n",
       " 'rose',\n",
       " 'thorn apple',\n",
       " 'morning glory',\n",
       " 'passion flower',\n",
       " 'lotus',\n",
       " 'toad lily',\n",
       " 'anthurium',\n",
       " 'frangipani',\n",
       " 'clematis',\n",
       " 'hibiscus',\n",
       " 'columbine',\n",
       " 'desert-rose',\n",
       " 'tree mallow',\n",
       " 'magnolia',\n",
       " 'cyclamen',\n",
       " 'watercress',\n",
       " 'canna lily',\n",
       " 'hippeastrum',\n",
       " 'bee balm',\n",
       " 'ball moss',\n",
       " 'foxglove',\n",
       " 'bougainvillea',\n",
       " 'camellia',\n",
       " 'mallow',\n",
       " 'mexican petunia',\n",
       " 'bromelia',\n",
       " 'blanket flower',\n",
       " 'trumpet creeper',\n",
       " 'blackberry lily']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:54.558657Z",
     "start_time": "2025-10-27T15:55:54.556171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 3. DataLoader ----------\n",
    "# ---------- Process of images is put on epoch loops\n",
    "def collate_pil(batch):\n",
    "    # batch: List[ (PIL.Image.Image, int) ]\n",
    "    images, labels = zip(*batch)           # images: tuple of PIL, labels: tuple of int\n",
    "    return list(images), torch.tensor(labels)  # 让 processor 接收 list[PIL]，labels 变成 LongTensor\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    "    #workers should be 4, but got problems in notebook\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_pil\n",
    ")"
   ],
   "id": "e55416c71fb1fdb0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Model Setting and Training"
   ],
   "id": "ea3ab3c785fcbdf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:56.299008Z",
     "start_time": "2025-10-27T15:55:54.626719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Word Prompt Embedding\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "promptTemplate = {\n",
    "    \"A photo of {}.\",\n",
    "    \"A photo of flower {}.\",\n",
    "    \"Botanic picture of {}\",\n",
    "    \"A example picture of type {}\"\n",
    "}\n",
    "# Use more templates to reduce sensitivity to other contexts\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(names):\n",
    "    embs = []\n",
    "    for name in tqdm(names, desc=\"TextEmbed\"):\n",
    "        prompts = [t.format(name.replace(\"_\",\" \")) for t in promptTemplate] # insert class names\n",
    "        inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        te = clip_model.get_text_features(**inputs)     # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        embs.append(te.mean(dim=0))                     # [D]\n",
    "    text = torch.stack(embs, dim=0)                     # [C, D]\n",
    "    return text / text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embs = build_text_embeddings(classname)          # [102, D], 固定不训练\n",
    "\n"
   ],
   "id": "2d9682cb89ba6dc0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextEmbed: 100%|██████████| 102/102 [00:01<00:00, 61.78it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Building CLIP model with LoRA and word embedding.\n",
    "\n",
    "Have to implement a LoRA linear layer ourselves."
   ],
   "id": "c5246531ba1768ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:56.360749Z",
     "start_time": "2025-10-27T15:55:56.356798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LoRA Injection (Trains LoRA matries only)\n",
    "import torch, torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "\n",
    "# y = w0 + x*(BA)*alpha/rank\n",
    "# Shape of A: din by rank / Shape of B: rank by dout\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r=8, alpha=16, dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.base = base # linear layer frozen for training LoRA parameters\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(base.in_features, r, bias=False)\n",
    "            self.lora_B = nn.Linear(r, base.out_features, bias=False)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight,a=5**0.5)\n",
    "            nn.init.zeros_(self.lora_B.weight) # set B to 0, avoid any bias introduced.\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "            #Frozen\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return self.base(x)\n",
    "\n",
    "\n",
    "# LoRA Injection with warped LoRA layer shown above.\n",
    "\n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler\n"
   ],
   "id": "63a9ba3e82edb850",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:56.421739Z",
     "start_time": "2025-10-27T15:55:56.413897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "head, optimizer, scaler = build_head_and_optim(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_image_feats(images):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    feats = clip_model.get_image_features(**inputs)           # [B, D]\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()"
   ],
   "id": "8a444b54cc7e7aa4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Main Training Epoch"
   ],
   "id": "ecf8ad5eafb9cbda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:55:56.478912Z",
     "start_time": "2025-10-27T15:55:56.475644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = get_image_feats(images)                   # [B, D]\n",
    "\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "\n",
    "    return {\n",
    "    \"loss\": loss_sum/total,\n",
    "    \"acc_cls\": correct_cls/total,   # 线性头准确率\n",
    "    \"acc_txt\": correct_txt/total,   # 文本读出准确率（zero-shot 风格）\n",
    "}\n",
    "\n"
   ],
   "id": "75d9230be58c43ff",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:56:34.149728Z",
     "start_time": "2025-10-27T15:55:56.539527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"[{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")\n"
   ],
   "id": "11a145942664fd64",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/32 [00:00<?, ?it/s]/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Train: 100%|██████████| 32/32 [00:32<00:00,  1.03s/it]\n",
      "Eval:  25%|██▌       | 8/32 [00:04<00:13,  1.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m ep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m,cfg.epochs+\u001B[32m1\u001B[39m):\n\u001B[32m      5\u001B[39m     training = run_epoch(train_loader, train=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m     val = \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mep\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg.epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      8\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTrain: loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_cls=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33macc_cls\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_txt=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33macc_txt\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVal:   loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_cls=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33macc_cls\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_txt=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33macc_txt\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m val[\u001B[33m\"\u001B[39m\u001B[33macc_cls\u001B[39m\u001B[33m\"\u001B[39m] > best_val:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mrun_epoch\u001B[39m\u001B[34m(loader, train)\u001B[39m\n\u001B[32m      9\u001B[39m total, correct_cls, correct_txt = \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m\n\u001B[32m     10\u001B[39m loss_sum = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mEval\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mamp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautocast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43menabled\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m==\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcuda\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mamp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    732\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    739\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    740\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    788\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    789\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m790\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    791\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    792\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torchvision/datasets/flowers102.py:87\u001B[39m, in \u001B[36mFlowers102.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx: \u001B[38;5;28mint\u001B[39m) -> \u001B[38;5;28mtuple\u001B[39m[Any, Any]:\n\u001B[32m     86\u001B[39m     image_file, label = \u001B[38;5;28mself\u001B[39m._image_files[idx], \u001B[38;5;28mself\u001B[39m._labels[idx]\n\u001B[32m---> \u001B[39m\u001B[32m87\u001B[39m     image = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform:\n\u001B[32m     90\u001B[39m         image = \u001B[38;5;28mself\u001B[39m.transform(image)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001B[39m, in \u001B[36mdefault_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    282\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m284\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torchvision/datasets/folder.py:263\u001B[39m, in \u001B[36mpil_loader\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpil_loader\u001B[39m(path: Union[\u001B[38;5;28mstr\u001B[39m, Path]) -> Image.Image:\n\u001B[32m    261\u001B[39m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[32m    262\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m--> \u001B[39m\u001B[32m263\u001B[39m         img = \u001B[43mImage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    264\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m img.convert(\u001B[33m\"\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/PIL/Image.py:3480\u001B[39m, in \u001B[36mopen\u001B[39m\u001B[34m(fp, mode, formats)\u001B[39m\n\u001B[32m   3477\u001B[39m     fp = io.BytesIO(fp.read())\n\u001B[32m   3478\u001B[39m     exclusive_fp = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3480\u001B[39m prefix = \u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   3482\u001B[39m preinit()\n\u001B[32m   3484\u001B[39m warning_messages: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] = []\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save trained model\n",
    "### Save head layer and LoRA layers only"
   ],
   "id": "1d0690dad522e224"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T16:32:16.041812Z",
     "start_time": "2025-10-27T16:32:16.036574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save Trained weights, as only head layer is trained\n",
    "def save_light_model(path,clip_model,head):\n",
    "    # sort out LoRA layers\n",
    "    lora_states = {k:v for k,v in clip_model.state_dict().items() if \"lora_\" in k}\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "        \"lora_states\": lora_states,\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Light weight model (only contains head and LoRA) saved to {path}\")\n",
    "\n",
    "\n",
    "    # Return clip_lora, head\n",
    "def load_light_simple(path, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    missing, unexpected = clip_model.load_state_dict(checkpoint[\"lora_states\"], strict=False) # set to false allow partial loading\n",
    "    print(missing, unexpected)\n",
    "\n",
    "    feat_dim = clip_model.config.projection_dim\n",
    "    head = nn.Linear(feat_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"[light] loaded ← {path}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Style Saving\n",
    "\n",
    "def save_full_dir(output_dir, clip_model, head):\n",
    "    output = Path(output_dir)\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    clip_model.save_pretrained(output)           # 保存到目录（含 LoRA 参数）\n",
    "    torch.save({\"num_classes\": head.out_features,\n",
    "                \"state_dict\": head.state_dict()}, output/\"head.pt\")\n",
    "    print(f\"[full-dir] saved → {output}\")\n",
    "\n",
    "def load_full_dir(output_dir, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    from transformers import CLIPModel\n",
    "    output = Path(output_dir)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(output).to(device)  # Load from folder\n",
    "    # 如你的保存目录包含了 LoRA 权重（因为它在 state_dict 里），这一步就不用再注入；\n",
    "    # 如果恢复后发现没有 LoRA 结构，可与上面相同：先注入再 load。\n",
    "\n",
    "    head_ckpt = torch.load(output/\"head.pt\", map_location=device)\n",
    "    import torch.nn as nn\n",
    "    head = nn.Linear(clip_model.config.projection_dim, head_ckpt[\"num_classes\"]).to(device)\n",
    "    head.load_state_dict(head_ckpt[\"state_dict\"])\n",
    "    print(f\"[full-dir] loaded ← {output}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Single File\n",
    "def save_full_model(path,clip_model,head):\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"clip_state_dict\": clip_model.state_dict(),\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Full  model  saved to {path}\")\n",
    "\n",
    "def load_full_model(path,device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    print(checkpoint[\"clip_name\"])\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    clip_model.load_state_dict(checkpoint[\"clip_state_dict\"], strict=True) # This time true cause loading full model\n",
    "\n",
    "    head = nn.Linear(clip_model.config.projection_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"Full  model  loaded from {path}\")\n",
    "    return clip_model, head\n"
   ],
   "id": "80d54e14fbbd12cd",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T15:57:25.732317Z",
     "start_time": "2025-10-27T15:57:25.688996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Actual Saving Code\n",
    "os.makedirs(\"model\",exist_ok=True)\n",
    "save_light_model(\"model/clip_weights.pt\", clip_model, head)"
   ],
   "id": "7b46319ea6fa5074",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light weight model (only contains head and LoRA) saved to model/clip_weights.pt\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T16:14:11.836264Z",
     "start_time": "2025-10-27T16:14:07.933379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try loading one\n",
    "clip_model, head = load_light_simple(\"model/clip_weights.pt\")\n",
    "\n",
    "from itertools import islice\n",
    "clip_model.eval()\n",
    "head.eval()\n",
    "processor = CLIPProcessor.from_pretrained(getattr(clip_model, \"name_or_path\", \"openai/clip-vit-base-patch32\"))\n",
    "\n",
    "def collate_pil(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "val_set  = datasets.Flowers102(root=\"./data\", split=\"val\",  download=True)\n",
    "test_set = datasets.Flowers102(root=\"./data\", split=\"test\", download=True)\n",
    "val_loader  = DataLoader(val_set,  batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_first(loader):\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        feats = clip_model.get_image_features(**inputs)          # [B, D]\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        logits = head(feats)                                     # [B, C]\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc  = evaluate_first(val_loader)\n",
    "test_acc = evaluate_first(test_loader)\n",
    "print(f\"Val Acc = {val_acc:.4f} | Test Acc = {test_acc:.4f}\")"
   ],
   "id": "62f1c97dc88c2105",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['logit_scale', 'text_model.embeddings.token_embedding.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.final_layer_norm.weight', 'text_model.final_layer_norm.bias', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.base.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.base.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.base.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.post_layernorm.bias', 'visual_projection.weight', 'text_projection.weight'] []\n",
      "[light] loaded ← model/clip_weights.pt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 33\u001B[39m\n\u001B[32m     29\u001B[39m         total   += labels.size(\u001B[32m0\u001B[39m)\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m correct / total\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m val_acc  = \u001B[43mevaluate_first\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m test_acc = evaluate_first(test_loader)\n\u001B[32m     35\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVal Acc = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Test Acc = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 29\u001B[39m, in \u001B[36mevaluate_first\u001B[39m\u001B[34m(loader)\u001B[39m\n\u001B[32m     27\u001B[39m     pred = logits.argmax(dim=-\u001B[32m1\u001B[39m)\n\u001B[32m     28\u001B[39m     correct += (pred == labels).sum().item()\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m     total   += \u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m correct / total\n",
      "\u001B[31mIndexError\u001B[39m: Dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T16:32:24.305874Z",
     "start_time": "2025-10-27T16:32:20.307411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This block saves full model\n",
    "\n",
    "save_full_model(\"model/full_model.pt\", clip_model, head)"
   ],
   "id": "eabd4d4d884e5851",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full  model  saved to model/full_model.pt\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T16:32:27.636269Z",
     "start_time": "2025-10-27T16:32:25.431494Z"
    }
   },
   "cell_type": "code",
   "source": "clip_model, head = load_full_model(\"model/full_model.pt\")",
   "id": "8fe60afe0cc38d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CLIPModel:\n\tMissing key(s) in state_dict: \"vision_model.encoder.layers.0.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.lora_B.weight\". \n\tUnexpected key(s) in state_dict: \"vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[50]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m clip_model, head = \u001B[43mload_full_model\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel/full_model.pt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[48]\u001B[39m\u001B[32m, line 90\u001B[39m, in \u001B[36mload_full_model\u001B[39m\u001B[34m(path, device, lora_targets)\u001B[39m\n\u001B[32m     88\u001B[39m lora_injection(clip_model, target_names=lora_targets)\n\u001B[32m     89\u001B[39m \u001B[38;5;66;03m# Load weights to injected layers\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[43mclip_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mclip_state_dict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# This time true cause loading full model\u001B[39;00m\n\u001B[32m     92\u001B[39m head = nn.Linear(clip_model.config.projection_dim,checkpoint[\u001B[33m\"\u001B[39m\u001B[33mnum_of_classes\u001B[39m\u001B[33m\"\u001B[39m]).to(device)\n\u001B[32m     93\u001B[39m head.load_state_dict(checkpoint[\u001B[33m\"\u001B[39m\u001B[33mhead_state_dict\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:2624\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2616\u001B[39m         error_msgs.insert(\n\u001B[32m   2617\u001B[39m             \u001B[32m0\u001B[39m,\n\u001B[32m   2618\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2619\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[32m   2620\u001B[39m             ),\n\u001B[32m   2621\u001B[39m         )\n\u001B[32m   2623\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2624\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2625\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2626\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)\n\u001B[32m   2627\u001B[39m         )\n\u001B[32m   2628\u001B[39m     )\n\u001B[32m   2629\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for CLIPModel:\n\tMissing key(s) in state_dict: \"vision_model.encoder.layers.0.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.0.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.1.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.2.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.3.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.4.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.5.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.6.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.7.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.8.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.9.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.10.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.k_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.v_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.q_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.lora_B.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.base.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.base.bias\", \"vision_model.encoder.layers.11.self_attn.out_proj.lora_A.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.lora_B.weight\". \n\tUnexpected key(s) in state_dict: \"vision_model.encoder.layers.0.self_attn.k_proj.weight\", \"vision_model.encoder.layers.0.self_attn.k_proj.bias\", \"vision_model.encoder.layers.0.self_attn.v_proj.weight\", \"vision_model.encoder.layers.0.self_attn.v_proj.bias\", \"vision_model.encoder.layers.0.self_attn.q_proj.weight\", \"vision_model.encoder.layers.0.self_attn.q_proj.bias\", \"vision_model.encoder.layers.0.self_attn.out_proj.weight\", \"vision_model.encoder.layers.0.self_attn.out_proj.bias\", \"vision_model.encoder.layers.1.self_attn.k_proj.weight\", \"vision_model.encoder.layers.1.self_attn.k_proj.bias\", \"vision_model.encoder.layers.1.self_attn.v_proj.weight\", \"vision_model.encoder.layers.1.self_attn.v_proj.bias\", \"vision_model.encoder.layers.1.self_attn.q_proj.weight\", \"vision_model.encoder.layers.1.self_attn.q_proj.bias\", \"vision_model.encoder.layers.1.self_attn.out_proj.weight\", \"vision_model.encoder.layers.1.self_attn.out_proj.bias\", \"vision_model.encoder.layers.2.self_attn.k_proj.weight\", \"vision_model.encoder.layers.2.self_attn.k_proj.bias\", \"vision_model.encoder.layers.2.self_attn.v_proj.weight\", \"vision_model.encoder.layers.2.self_attn.v_proj.bias\", \"vision_model.encoder.layers.2.self_attn.q_proj.weight\", \"vision_model.encoder.layers.2.self_attn.q_proj.bias\", \"vision_model.encoder.layers.2.self_attn.out_proj.weight\", \"vision_model.encoder.layers.2.self_attn.out_proj.bias\", \"vision_model.encoder.layers.3.self_attn.k_proj.weight\", \"vision_model.encoder.layers.3.self_attn.k_proj.bias\", \"vision_model.encoder.layers.3.self_attn.v_proj.weight\", \"vision_model.encoder.layers.3.self_attn.v_proj.bias\", \"vision_model.encoder.layers.3.self_attn.q_proj.weight\", \"vision_model.encoder.layers.3.self_attn.q_proj.bias\", \"vision_model.encoder.layers.3.self_attn.out_proj.weight\", \"vision_model.encoder.layers.3.self_attn.out_proj.bias\", \"vision_model.encoder.layers.4.self_attn.k_proj.weight\", \"vision_model.encoder.layers.4.self_attn.k_proj.bias\", \"vision_model.encoder.layers.4.self_attn.v_proj.weight\", \"vision_model.encoder.layers.4.self_attn.v_proj.bias\", \"vision_model.encoder.layers.4.self_attn.q_proj.weight\", \"vision_model.encoder.layers.4.self_attn.q_proj.bias\", \"vision_model.encoder.layers.4.self_attn.out_proj.weight\", \"vision_model.encoder.layers.4.self_attn.out_proj.bias\", \"vision_model.encoder.layers.5.self_attn.k_proj.weight\", \"vision_model.encoder.layers.5.self_attn.k_proj.bias\", \"vision_model.encoder.layers.5.self_attn.v_proj.weight\", \"vision_model.encoder.layers.5.self_attn.v_proj.bias\", \"vision_model.encoder.layers.5.self_attn.q_proj.weight\", \"vision_model.encoder.layers.5.self_attn.q_proj.bias\", \"vision_model.encoder.layers.5.self_attn.out_proj.weight\", \"vision_model.encoder.layers.5.self_attn.out_proj.bias\", \"vision_model.encoder.layers.6.self_attn.k_proj.weight\", \"vision_model.encoder.layers.6.self_attn.k_proj.bias\", \"vision_model.encoder.layers.6.self_attn.v_proj.weight\", \"vision_model.encoder.layers.6.self_attn.v_proj.bias\", \"vision_model.encoder.layers.6.self_attn.q_proj.weight\", \"vision_model.encoder.layers.6.self_attn.q_proj.bias\", \"vision_model.encoder.layers.6.self_attn.out_proj.weight\", \"vision_model.encoder.layers.6.self_attn.out_proj.bias\", \"vision_model.encoder.layers.7.self_attn.k_proj.weight\", \"vision_model.encoder.layers.7.self_attn.k_proj.bias\", \"vision_model.encoder.layers.7.self_attn.v_proj.weight\", \"vision_model.encoder.layers.7.self_attn.v_proj.bias\", \"vision_model.encoder.layers.7.self_attn.q_proj.weight\", \"vision_model.encoder.layers.7.self_attn.q_proj.bias\", \"vision_model.encoder.layers.7.self_attn.out_proj.weight\", \"vision_model.encoder.layers.7.self_attn.out_proj.bias\", \"vision_model.encoder.layers.8.self_attn.k_proj.weight\", \"vision_model.encoder.layers.8.self_attn.k_proj.bias\", \"vision_model.encoder.layers.8.self_attn.v_proj.weight\", \"vision_model.encoder.layers.8.self_attn.v_proj.bias\", \"vision_model.encoder.layers.8.self_attn.q_proj.weight\", \"vision_model.encoder.layers.8.self_attn.q_proj.bias\", \"vision_model.encoder.layers.8.self_attn.out_proj.weight\", \"vision_model.encoder.layers.8.self_attn.out_proj.bias\", \"vision_model.encoder.layers.9.self_attn.k_proj.weight\", \"vision_model.encoder.layers.9.self_attn.k_proj.bias\", \"vision_model.encoder.layers.9.self_attn.v_proj.weight\", \"vision_model.encoder.layers.9.self_attn.v_proj.bias\", \"vision_model.encoder.layers.9.self_attn.q_proj.weight\", \"vision_model.encoder.layers.9.self_attn.q_proj.bias\", \"vision_model.encoder.layers.9.self_attn.out_proj.weight\", \"vision_model.encoder.layers.9.self_attn.out_proj.bias\", \"vision_model.encoder.layers.10.self_attn.k_proj.weight\", \"vision_model.encoder.layers.10.self_attn.k_proj.bias\", \"vision_model.encoder.layers.10.self_attn.v_proj.weight\", \"vision_model.encoder.layers.10.self_attn.v_proj.bias\", \"vision_model.encoder.layers.10.self_attn.q_proj.weight\", \"vision_model.encoder.layers.10.self_attn.q_proj.bias\", \"vision_model.encoder.layers.10.self_attn.out_proj.weight\", \"vision_model.encoder.layers.10.self_attn.out_proj.bias\", \"vision_model.encoder.layers.11.self_attn.k_proj.weight\", \"vision_model.encoder.layers.11.self_attn.k_proj.bias\", \"vision_model.encoder.layers.11.self_attn.v_proj.weight\", \"vision_model.encoder.layers.11.self_attn.v_proj.bias\", \"vision_model.encoder.layers.11.self_attn.q_proj.weight\", \"vision_model.encoder.layers.11.self_attn.q_proj.bias\", \"vision_model.encoder.layers.11.self_attn.out_proj.weight\", \"vision_model.encoder.layers.11.self_attn.out_proj.bias\". "
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
