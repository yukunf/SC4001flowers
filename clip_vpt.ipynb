{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:04:56.015820Z",
     "start_time": "2025-10-31T08:04:55.997361Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "from itertools import islice\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import pipeline, CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "LOADER_PATCH_SIZE = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Cuda Availability:{torch.cuda.is_available()} Training on {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Availability:True Training on cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "c24885366b00e5e9",
   "metadata": {},
   "source": [
    "# Notebook for interactive testing for CLIP"
   ]
  },
  {
   "cell_type": "code",
   "id": "afdd8de3917c931a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:16:42.871124Z",
     "start_time": "2025-10-31T08:16:38.424807Z"
    }
   },
   "source": [
    "\n",
    "class Cfg:\n",
    "    model_id: str = \"openai/clip-vit-base-patch32\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    seed:   int = 42\n",
    "\n",
    "    # -------- Optim & Loss ----------\n",
    "    lr_head: float = 1e-3      # 线性头\n",
    "    wd_head: float = 1e-4\n",
    "    lr_lora: float = 1e-4      # LoRA 注入层\n",
    "    wd_lora: float = 1e-2\n",
    "    lambda_text: float = 0.3   # 文本对齐辅助损失权重\n",
    "    \n",
    "    early_stopping: bool = True\n",
    "    early_stop_patience = 3\n",
    "    early_stop_minimum_improvement:float = 0.05\n",
    "    \n",
    "\n",
    "    # -------- LoRA ----------\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_target: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")  # 只对注意力投影层做LoRA\n",
    "    # 也可扩展到 MLP 内部 proj，但注意稳定性\n",
    "\n",
    "    amp: bool = True\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "PREPROCESS_DATA_ROOT = \"data/preprocessed\"\n",
    "torch.manual_seed(cfg.seed)\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device).eval()\n",
    "processor  = CLIPProcessor.from_pretrained(model_id)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "2a9dfdb42fcf2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:04.408341Z",
     "start_time": "2025-10-31T08:05:01.152032Z"
    }
   },
   "source": [
    "# ---------- 2. Load Dataset ----------\n",
    "def collate_pil(batch):\n",
    "    # batch: List[ (PIL.Image.Image, int) ]\n",
    "    images, labels = zip(*batch)           # images: tuple of PIL, labels: tuple of int\n",
    "    return list(images), torch.tensor(labels)  # 让 processor 接收 list[PIL]，labels 变成 LongTensor\n",
    "\n",
    "def preprocess_dataset(split=\"train\",data_root='data',outputdir=\"data/preprocessed\",batchsize=cfg.batch_size,fp16=False):\n",
    "    os.makedirs(outputdir,exist_ok=True)\n",
    "    dataset = datasets.Flowers102(root=data_root, split=split, download=True)\n",
    "    classes = dataset.classes\n",
    "    loader = DataLoader(dataset, batch_size=batchsize, shuffle= True if split==\"train\" else False, num_workers=0,collate_fn=collate_pil)\n",
    "    \n",
    "    # Allocate RAM\n",
    "    N = len(dataset)\n",
    "    C, H, W = 3, 224, 224\n",
    "    pixels = torch.empty((N,C,H,W), dtype=torch.float32 if not fp16 else torch.float16)\n",
    "    labels = torch.empty(N, dtype=torch.long)\n",
    "    \n",
    "    print(f\"Preprocessing {split} data...\")\n",
    "    \n",
    "    index = 0\n",
    "    with torch.no_grad():\n",
    "        for images, y in tqdm(loader,desc=f\"Preprocessing {split}\"):\n",
    "            pix = processor(images=images,return_tensors=\"pt\")['pixel_values']\n",
    "            if fp16:\n",
    "                pix = pix.half()\n",
    "            b = pix.size(0) # patch size\n",
    "            pixels[index:index+b] = pix\n",
    "            labels[index:index+b] = y\n",
    "            index += b\n",
    "    \n",
    "    \n",
    "    out_path = os.path.join(outputdir, f\"{split}.pt\")\n",
    "    torch.save({\"pixel_values\": pixels, \"labels\": labels, \"fp16\": fp16,\"classes\":classes}, out_path)\n",
    "    print(f\"Saved → {out_path} (pixels: {pixels.shape}, dtype={pixels.dtype})\")\n",
    "    \n",
    "    print(\"Performing Garbage Cleaning...\")\n",
    "    del pixels, labels\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # for notebook\n",
    "    %reset_selective -f pixels\n",
    "    %reset_selective -f labels\n",
    "\n",
    "class CacheDataset:\n",
    "    def __init__(self, split=\"train\", root=\"data/preprocessed\"):\n",
    "        path = os.path.join(root, f\"{split}.pt\")\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "        self.pixel_values = obj[\"pixel_values\"]\n",
    "        self.labels = obj[\"labels\"]\n",
    "        self.fp16 = bool(obj.get(\"fp16\", False))\n",
    "        self.classes = obj['classes']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.labels.numel()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pixel_values[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "# Preprocess and save dataset                \n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"train.pt\")):    \n",
    "    preprocess_dataset(split=\"train\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"val.pt\")):  \n",
    "    preprocess_dataset(split=\"val\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "if not os.path.exists(os.path.join(PREPROCESS_DATA_ROOT, \"test.pt\")):  \n",
    "    preprocess_dataset(split=\"test\",data_root=\"data\",outputdir=PREPROCESS_DATA_ROOT,fp16=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Load dataset\n",
    "train_set = CacheDataset(split=\"train\")\n",
    "val_set = CacheDataset(split=\"val\")\n",
    "test_set = CacheDataset(split=\"test\")\n",
    "\n",
    "classname = val_set.classes\n",
    "classname\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pink primrose',\n",
       " 'hard-leaved pocket orchid',\n",
       " 'canterbury bells',\n",
       " 'sweet pea',\n",
       " 'english marigold',\n",
       " 'tiger lily',\n",
       " 'moon orchid',\n",
       " 'bird of paradise',\n",
       " 'monkshood',\n",
       " 'globe thistle',\n",
       " 'snapdragon',\n",
       " \"colt's foot\",\n",
       " 'king protea',\n",
       " 'spear thistle',\n",
       " 'yellow iris',\n",
       " 'globe-flower',\n",
       " 'purple coneflower',\n",
       " 'peruvian lily',\n",
       " 'balloon flower',\n",
       " 'giant white arum lily',\n",
       " 'fire lily',\n",
       " 'pincushion flower',\n",
       " 'fritillary',\n",
       " 'red ginger',\n",
       " 'grape hyacinth',\n",
       " 'corn poppy',\n",
       " 'prince of wales feathers',\n",
       " 'stemless gentian',\n",
       " 'artichoke',\n",
       " 'sweet william',\n",
       " 'carnation',\n",
       " 'garden phlox',\n",
       " 'love in the mist',\n",
       " 'mexican aster',\n",
       " 'alpine sea holly',\n",
       " 'ruby-lipped cattleya',\n",
       " 'cape flower',\n",
       " 'great masterwort',\n",
       " 'siam tulip',\n",
       " 'lenten rose',\n",
       " 'barbeton daisy',\n",
       " 'daffodil',\n",
       " 'sword lily',\n",
       " 'poinsettia',\n",
       " 'bolero deep blue',\n",
       " 'wallflower',\n",
       " 'marigold',\n",
       " 'buttercup',\n",
       " 'oxeye daisy',\n",
       " 'common dandelion',\n",
       " 'petunia',\n",
       " 'wild pansy',\n",
       " 'primula',\n",
       " 'sunflower',\n",
       " 'pelargonium',\n",
       " 'bishop of llandaff',\n",
       " 'gaura',\n",
       " 'geranium',\n",
       " 'orange dahlia',\n",
       " 'pink-yellow dahlia?',\n",
       " 'cautleya spicata',\n",
       " 'japanese anemone',\n",
       " 'black-eyed susan',\n",
       " 'silverbush',\n",
       " 'californian poppy',\n",
       " 'osteospermum',\n",
       " 'spring crocus',\n",
       " 'bearded iris',\n",
       " 'windflower',\n",
       " 'tree poppy',\n",
       " 'gazania',\n",
       " 'azalea',\n",
       " 'water lily',\n",
       " 'rose',\n",
       " 'thorn apple',\n",
       " 'morning glory',\n",
       " 'passion flower',\n",
       " 'lotus',\n",
       " 'toad lily',\n",
       " 'anthurium',\n",
       " 'frangipani',\n",
       " 'clematis',\n",
       " 'hibiscus',\n",
       " 'columbine',\n",
       " 'desert-rose',\n",
       " 'tree mallow',\n",
       " 'magnolia',\n",
       " 'cyclamen',\n",
       " 'watercress',\n",
       " 'canna lily',\n",
       " 'hippeastrum',\n",
       " 'bee balm',\n",
       " 'ball moss',\n",
       " 'foxglove',\n",
       " 'bougainvillea',\n",
       " 'camellia',\n",
       " 'mallow',\n",
       " 'mexican petunia',\n",
       " 'bromelia',\n",
       " 'blanket flower',\n",
       " 'trumpet creeper',\n",
       " 'blackberry lily']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:04.410556Z",
     "start_time": "2025-10-31T08:05:04.408341Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1a8294a5bd088ae",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e55416c71fb1fdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:04.414319Z",
     "start_time": "2025-10-31T08:05:04.410556Z"
    }
   },
   "source": [
    "# ---------- 3. DataLoader ----------\n",
    "# ---------- Process of images is put on epoch loops\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True\n",
    "    #workers should be 4, but got problems in notebook\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ea3ab3c785fcbdf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Setting and Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d9682cb89ba6dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:05.577447Z",
     "start_time": "2025-10-31T08:05:04.414319Z"
    }
   },
   "source": [
    "# Word Prompt Embedding\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "promptTemplate = {\n",
    "    \"A photo of {}.\",\n",
    "    \"A photo of flower {}.\",\n",
    "    \"Botanic picture of {}\",\n",
    "    \"A example picture of type {}\"\n",
    "}\n",
    "# Use more templates to reduce sensitivity to other contexts\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(names):\n",
    "    embs = []\n",
    "    for name in tqdm(names, desc=\"TextEmbed\"):\n",
    "        prompts = [t.format(name.replace(\"_\",\" \")) for t in promptTemplate] # insert class names\n",
    "        inputs = processor(text=prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        te = clip_model.get_text_features(**inputs)     # [T, D]\n",
    "        te = te / te.norm(dim=-1, keepdim=True)\n",
    "        embs.append(te.mean(dim=0))                     # [D]\n",
    "    text = torch.stack(embs, dim=0)                     # [C, D]\n",
    "    return text / text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embs = build_text_embeddings(classname)          # [102, D], 固定不训练\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TextEmbed: 100%|██████████| 102/102 [00:01<00:00, 88.61it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c5246531ba1768ef",
   "metadata": {},
   "source": [
    "----\n",
    "## Building CLIP model with LoRA and word embedding.\n",
    "\n",
    "Have to implement a LoRA linear layer ourselves."
   ]
  },
  {
   "cell_type": "code",
   "id": "63a9ba3e82edb850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:05.585593Z",
     "start_time": "2025-10-31T08:05:05.577447Z"
    }
   },
   "source": [
    "# LoRA Injection (Trains LoRA matries only)\n",
    "import torch, torch.nn as nn\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "\n",
    "# y = w0 + x*(BA)*alpha/rank\n",
    "# Shape of A: din by rank / Shape of B: rank by dout\n",
    "class LoRALinearLayer(nn.Module):\n",
    "    def __init__(self, base: nn.Linear, r=8, alpha=16, dropout=0.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.base = base # linear layer frozen for training LoRA parameters\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "        dev = base.weight.device\n",
    "        dt  = base.weight.dtype\n",
    "\n",
    "        \n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Linear(base.in_features, r, bias=False).to(dev, dtype=dt)\n",
    "            self.lora_B = nn.Linear(r, base.out_features, bias=False).to(dev, dtype=dt)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight,a=5**0.5)\n",
    "            nn.init.zeros_(self.lora_B.weight) # set B to 0, avoid any bias introduced.\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "            #Frozen\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.r > 0:\n",
    "            return self.base(x) + self.dropout(self.lora_B(self.lora_A(x))) * self.scaling\n",
    "        else:\n",
    "            return self.base(x)\n",
    "\n",
    "\n",
    "# LoRA Injection with warped LoRA layer shown above.\n",
    "\n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "    clip_model.to(device)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "8a444b54cc7e7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:05:05.612309Z",
     "start_time": "2025-10-31T08:05:05.585593Z"
    }
   },
   "source": [
    "head, optimizer, scaler = build_head_and_optim(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def get_image_feats(images):\n",
    "    # images are actual tensor now\n",
    "    # assume it's preprocessed\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        pixel_values = images.to(device, dtype=torch.float16 if (device==\"cuda\" and cfg.amp) else torch.float32)\n",
    "    else:\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        \n",
    "    feats = clip_model.get_image_features(pixel_values=pixel_values)           # [B, D]\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ecf8ad5eafb9cbda",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Main Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "id": "75d9230be58c43ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:07:48.751953Z",
     "start_time": "2025-10-31T08:07:48.740786Z"
    }
   },
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.last_loss = 0\n",
    "        self.patience = cfg.early_stop_patience\n",
    "        self.enable = cfg.early_stopping\n",
    "        self.delta = cfg.early_stop_minimum_improvement\n",
    "    def report(self,loss):\n",
    "        if self.last_loss - loss < self.delta:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        self.last_loss = loss\n",
    "    \n",
    "    def stop_flag(self):\n",
    "        return self.enable and (self.counter >= self.patience)\n",
    "        \n",
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = get_image_feats(images)                   # [B, D]\n",
    "\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "        \n",
    "    loss_avg = loss_sum / total\n",
    "    \n",
    "    return {\n",
    "    \"loss\": loss_avg,\n",
    "    \"acc_cls\": correct_cls/total,   # 线性头准确率\n",
    "    \"acc_txt\": correct_txt/total,   # 文本读出准确率（zero-shot 风格）\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "11a145942664fd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:07:55.061827Z",
     "start_time": "2025-10-31T08:07:52.532153Z"
    }
   },
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "earlystop = EarlyStopper()\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"[{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "    earlystop.report(val['loss'])\n",
    "    if earlystop.stop_flag():\n",
    "        print(f\"Early stop triggered...Exiting on epoch {ep}\")\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  41%|████      | 13/32 [00:02<00:03,  5.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m earlystop = EarlyStopper()\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m ep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m,cfg.epochs+\u001B[32m1\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     training = \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     val = run_epoch(val_loader, train=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      7\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mep\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg.epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      8\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTrain: loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_cls=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33macc_cls\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_txt=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtraining[\u001B[33m'\u001B[39m\u001B[33macc_txt\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      9\u001B[39m           \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVal:   loss=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_cls=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33macc_cls\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m acc_txt=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval[\u001B[33m'\u001B[39m\u001B[33macc_txt\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36mrun_epoch\u001B[39m\u001B[34m(loader, train)\u001B[39m\n\u001B[32m     45\u001B[39m     optimizer.zero_grad()\n\u001B[32m     46\u001B[39m     scaler.scale(loss).backward()\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m     \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m     scaler.update()\n\u001B[32m     51\u001B[39m \u001B[38;5;66;03m#Stats\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:462\u001B[39m, in \u001B[36mGradScaler.step\u001B[39m\u001B[34m(self, optimizer, *args, **kwargs)\u001B[39m\n\u001B[32m    456\u001B[39m     \u001B[38;5;28mself\u001B[39m.unscale_(optimizer)\n\u001B[32m    458\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(optimizer_state[\u001B[33m\"\u001B[39m\u001B[33mfound_inf_per_device\u001B[39m\u001B[33m\"\u001B[39m]) > \u001B[32m0\u001B[39m, (\n\u001B[32m    459\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mNo inf checks were recorded for this optimizer.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    460\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m462\u001B[39m retval = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_maybe_opt_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    464\u001B[39m optimizer_state[\u001B[33m\"\u001B[39m\u001B[33mstage\u001B[39m\u001B[33m\"\u001B[39m] = OptState.STEPPED\n\u001B[32m    466\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:356\u001B[39m, in \u001B[36mGradScaler._maybe_opt_step\u001B[39m\u001B[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001B[39m\n\u001B[32m    348\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_maybe_opt_step\u001B[39m(\n\u001B[32m    349\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    350\u001B[39m     optimizer: torch.optim.Optimizer,\n\u001B[32m   (...)\u001B[39m\u001B[32m    353\u001B[39m     **kwargs: Any,\n\u001B[32m    354\u001B[39m ) -> Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    355\u001B[39m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m356\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moptimizer_state\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf_per_device\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    357\u001B[39m         retval = optimizer.step(*args, **kwargs)\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\sc4001\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:356\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    348\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_maybe_opt_step\u001B[39m(\n\u001B[32m    349\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    350\u001B[39m     optimizer: torch.optim.Optimizer,\n\u001B[32m   (...)\u001B[39m\u001B[32m    353\u001B[39m     **kwargs: Any,\n\u001B[32m    354\u001B[39m ) -> Optional[\u001B[38;5;28mfloat\u001B[39m]:\n\u001B[32m    355\u001B[39m     retval: Optional[\u001B[38;5;28mfloat\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m356\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[43mv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m optimizer_state[\u001B[33m\"\u001B[39m\u001B[33mfound_inf_per_device\u001B[39m\u001B[33m\"\u001B[39m].values()):\n\u001B[32m    357\u001B[39m         retval = optimizer.step(*args, **kwargs)\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m retval\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "1d0690dad522e224",
   "metadata": {},
   "source": [
    "## Save trained model\n",
    "### Save head layer and LoRA layers only"
   ]
  },
  {
   "cell_type": "code",
   "id": "80d54e14fbbd12cd",
   "metadata": {},
   "source": [
    "# Save Trained weights, as only head layer is trained\n",
    "def save_light_model(path,clip_model,head):\n",
    "    # sort out LoRA layers\n",
    "    lora_states = {k:v for k,v in clip_model.state_dict().items() if \"lora_\" in k}\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "        \"lora_states\": lora_states,\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Light weight model (only contains head and LoRA) saved to {path}\")\n",
    "\n",
    "\n",
    "    # Return clip_lora, head\n",
    "def load_light_simple(path, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    missing, unexpected = clip_model.load_state_dict(checkpoint[\"lora_states\"], strict=False) # set to false allow partial loading\n",
    "    print(missing, unexpected)\n",
    "\n",
    "    feat_dim = clip_model.config.projection_dim\n",
    "    head = nn.Linear(feat_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"[light] loaded ← {path}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Style Saving\n",
    "\n",
    "def save_full_dir(output_dir, clip_model, head):\n",
    "    output = Path(output_dir)\n",
    "    output.mkdir(parents=True, exist_ok=True)\n",
    "    clip_model.save_pretrained(output)           # 保存到目录（含 LoRA 参数）\n",
    "    torch.save({\"num_classes\": head.out_features,\n",
    "                \"state_dict\": head.state_dict()}, output/\"head.pt\")\n",
    "    print(f\"[full-dir] saved → {output}\")\n",
    "\n",
    "def load_full_dir(output_dir, device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    from transformers import CLIPModel\n",
    "    output = Path(output_dir)\n",
    "\n",
    "    clip_model = CLIPModel.from_pretrained(output).to(device)  # Load from folder\n",
    "    # 如你的保存目录包含了 LoRA 权重（因为它在 state_dict 里），这一步就不用再注入；\n",
    "    # 如果恢复后发现没有 LoRA 结构，可与上面相同：先注入再 load。\n",
    "\n",
    "    head_ckpt = torch.load(output/\"head.pt\", map_location=device)\n",
    "    import torch.nn as nn\n",
    "    head = nn.Linear(clip_model.config.projection_dim, head_ckpt[\"num_classes\"]).to(device)\n",
    "    head.load_state_dict(head_ckpt[\"state_dict\"])\n",
    "    print(f\"[full-dir] loaded ← {output}\")\n",
    "    return clip_model, head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Single File\n",
    "def save_full_model(path,clip_model,head):\n",
    "    checkpoint = {\n",
    "        \"clip_name\": \"openai/clip-vit-base-patch32\",\n",
    "        \"num_of_classes\": head.out_features,\n",
    "        \"clip_state_dict\": clip_model.state_dict(),\n",
    "        \"head_state_dict\": head.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint,path)\n",
    "    print(f\"Full  model  saved to {path}\")\n",
    "\n",
    "def load_full_model(path,device=device, lora_targets=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "    print(checkpoint[\"clip_name\"])\n",
    "    clip_model = CLIPModel.from_pretrained(checkpoint[\"clip_name\"]).to(device)\n",
    "\n",
    "    # Injection again\n",
    "    lora_injection(clip_model, target_names=lora_targets)\n",
    "    # Load weights to injected layers\n",
    "    clip_model.load_state_dict(checkpoint[\"clip_state_dict\"], strict=True) # This time true cause loading full model\n",
    "\n",
    "    head = nn.Linear(clip_model.config.projection_dim,checkpoint[\"num_of_classes\"]).to(device)\n",
    "    head.load_state_dict(checkpoint[\"head_state_dict\"])\n",
    "\n",
    "    print(f\"Full  model  loaded from {path}\")\n",
    "    return clip_model, head\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b46319ea6fa5074",
   "metadata": {},
   "source": [
    "# Actual Saving Code\n",
    "os.makedirs(\"model\",exist_ok=True)\n",
    "save_light_model(\"model/clip_weights.pt\", clip_model, head)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62f1c97dc88c2105",
   "metadata": {},
   "source": [
    "# Try loading one\n",
    "clip_model, head = load_light_simple(\"model/clip_weights.pt\")\n",
    "\n",
    "from itertools import islice\n",
    "clip_model.eval()\n",
    "head.eval()\n",
    "processor = CLIPProcessor.from_pretrained(getattr(clip_model, \"name_or_path\", \"openai/clip-vit-base-patch32\"))\n",
    "\n",
    "def collate_pil(batch):\n",
    "    imgs, labels = zip(*batch)\n",
    "    return list(imgs), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "val_set  = datasets.Flowers102(root=\"./data\", split=\"val\",  download=True)\n",
    "test_set = datasets.Flowers102(root=\"./data\", split=\"test\", download=True)\n",
    "val_loader  = DataLoader(val_set,  batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_pil)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_first(loader):\n",
    "    total, correct = 0, 0\n",
    "    for images, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        feats = clip_model.get_image_features(**inputs)          # [B, D]\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        logits = head(feats)                                     # [B, C]\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc  = evaluate_first(val_loader)\n",
    "test_acc = evaluate_first(test_loader)\n",
    "print(f\"Val Acc = {val_acc:.4f} | Test Acc = {test_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eabd4d4d884e5851",
   "metadata": {},
   "source": [
    "# This block saves full model\n",
    "\n",
    "# save_full_model(\"model/full_model.pt\", clip_model, head)\n",
    "# clip_model, head = load_full_model(\"model/full_model.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "----\n",
    "## Fine-Tuning of CLIP: Visual Prompt Tuning\n",
    "\n",
    "We insert a learnable prompt before encoder of transformer, along with the images."
   ],
   "id": "d378522a8563fc31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:11:31.299490Z",
     "start_time": "2025-10-31T08:11:31.289186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Config\n",
    "VISUAL_PROMPT_SIZE = 16\n",
    "\n",
    "class VPTShallowClip(nn.Module):\n",
    "    def __init__(self, clip_model: CLIPModel, init_std: float = 0.02, freeze_backbone: bool = True):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        vm = self.clip.vision_model\n",
    "        D = vm.config.hidden_size # Hidden size of embedding\n",
    "        m = VISUAL_PROMPT_SIZE\n",
    "        self.prompt =   nn.Parameter(torch.randn(m, D) * init_std)\n",
    "        self.prompt_pos = nn.Parameter(torch.zeros(1,m, D)) # Positional Encoding, each one is different thus need B different ones.\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for p in self.clip.vision_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.clip.visual_projection.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def embed_with_pos(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # reuse\n",
    "        return self.clip.vision_model.embeddings(pixel_values)\n",
    "    \n",
    "    def _encode_with_prompts(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # CLS+ img patch = [B, 1+N, D]\n",
    "    # we add prompt and prompt_pos into the vector before the actual image.\n",
    "    \n",
    "        vm = self.clip.vision_model\n",
    "        B = x.size(0)\n",
    "        cls, patches = x[:,:1, :], x[:,1:,:]\n",
    "        p = self.prompt + self.prompt_pos # positional embedding\n",
    "        p = p.expand(B, -1, -1)\n",
    "    \n",
    "        x0 = torch.cat([cls, p, patches], dim=1)\n",
    "        \n",
    "        # send to model for embedding\n",
    "        h = vm.pre_layrnorm(x0)\n",
    "        enc_out = vm.encoder(inputs_embeds=h)\n",
    "        last = enc_out.last_hidden_state                   # [B,1+m+N,D]\n",
    "        last = vm.post_layernorm(last)\n",
    "        \n",
    "        return last\n",
    "    \n",
    "    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # align with CLIPmodel.get_image_features to ensure code reusable\n",
    "        pixel_values = pixel_values.to(next(self.parameters()).device, dtype=next(self.parameters()).dtype)\n",
    "        \n",
    "        x = self.embed_with_pos(pixel_values) # [B,1+N,M]\n",
    "        \n",
    "        last = self._encode_with_prompts(x)\n",
    "        \n",
    "        pooled = last[:,0,:] # get cls [B,D]\n",
    "        \n",
    "        feats = self.clip.visual_projection(pooled)       # [B, projection_dim]\n",
    "        return feats\n",
    "        \n",
    "def lora_injection(clip_model: nn.Module, target_names=(\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\")):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(clip_model.vision_model, CLIPVisionModel.__mro__[0].__class__) or hasattr(clip_model, \"vision_model\")\n",
    "    lora_params = []\n",
    "    for name, module in clip_model.vision_model.named_modules():\n",
    "        # injection to clip/transformer attention layer: q_proj/k_proj/v_proj/out_proj\n",
    "        for t in target_names:\n",
    "            if hasattr(module, t):\n",
    "                lin = getattr(module, t)\n",
    "                if isinstance(lin, nn.Linear):\n",
    "                    lora_lin = LoRALinearLayer(lin, r=cfg.lora_rank, alpha=cfg.lora_alpha, dropout=cfg.lora_dropout)\n",
    "                    setattr(module, t, lora_lin)\n",
    "                    lora_params += list(lora_lin.lora_A.parameters()) + list(lora_lin.lora_B.parameters())\n",
    "    # Freeze the parameters\n",
    "    for p in clip_model.vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in lora_params:\n",
    "        p.requires_grad = True\n",
    "    return lora_params\n",
    "\n",
    "def build_head_and_optim_vpt(clip_model: CLIPModel):\n",
    "    feat_dim = clip_model.config.projection_dim  # ViT-B/32 = 512\n",
    "    head = nn.Linear(feat_dim, 102).to(device)\n",
    "\n",
    "    lora_params = lora_injection(clip_model, target_names=cfg.lora_target)\n",
    "    clip_model.to(device)\n",
    "\n",
    "    # 2 parameter groups: LoRA and linear head\n",
    "    optim = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": head.parameters(),      \"lr\": cfg.lr_head, \"weight_decay\": cfg.wd_head},\n",
    "            {\"params\": lora_params,            \"lr\": cfg.lr_lora, \"weight_decay\": cfg.wd_lora},\n",
    "            {\"params\": [vpt.prompt, vpt.prompt_pos], \"lr\": 5e-3, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    )\n",
    "    scaler = torch.amp.GradScaler(enabled=(device==\"cuda\" and cfg.amp))\n",
    "    return head, optim, scaler"
   ],
   "id": "eef4e448fdbd5fb2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:11:36.069337Z",
     "start_time": "2025-10-31T08:11:31.595686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Re init\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "vpt = VPTShallowClip(clip_model, init_std=0.02, freeze_backbone=True).to(device)\n",
    "\n",
    "head, optimizer, scaler = build_head_and_optim_vpt(clip_model)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def supervised_logits(feats):\n",
    "    return head(feats)                                        # [B, 102]\n",
    "\n",
    "def text_logits(feats):\n",
    "    # perform cosine similarity with text embedding.\n",
    "    return (feats @ text_embs.T) * clip_model.logit_scale.exp()\n",
    "\n",
    "def run_epoch(loader: DataLoader, train: bool=True):\n",
    "    if train:\n",
    "        head.train()\n",
    "        clip_model.train()\n",
    "    else:\n",
    "        head.eval()\n",
    "        clip_model.eval()\n",
    "\n",
    "    total, correct_cls, correct_txt = 0, 0, 0\n",
    "    loss_sum = 0.0\n",
    "    for images, labels in tqdm(loader, desc=\"Train\" if train else \"Eval\"):\n",
    "        labels = labels.to(device)\n",
    "        with torch.amp.autocast(device_type=device,enabled=(device==\"cuda\" and cfg.amp)):\n",
    "            feats = vpt.get_image_features(pixel_values=images)                # [B, D]\n",
    "            feats  = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            logits_cls = supervised_logits(feats) # logits of classification score from linear layer head\n",
    "            loss_cls = ce(logits_cls, labels)\n",
    "\n",
    "            logits_txt = text_logits(feats) # logits of text embedding trained in transformer\n",
    "            loss_txt = ce(logits_txt, labels)\n",
    "            # Alignment between text and img.\n",
    "\n",
    "            loss = loss_cls + cfg.lambda_text * loss_txt # weighted\n",
    "\n",
    "\n",
    "        if train:\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "\n",
    "        #Stats\n",
    "\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "        total += labels.size(0)\n",
    "        correct_cls += (logits_cls.argmax(dim=-1) == labels).sum().item()\n",
    "        correct_txt += (logits_txt.argmax(dim=-1) == labels).sum().item()\n",
    "        \n",
    "    loss_avg = loss_sum / total\n",
    "    \n",
    "    return {\n",
    "    \"loss\": loss_avg,\n",
    "    \"acc_cls\": correct_cls/total,   # 线性头准确率\n",
    "    \"acc_txt\": correct_txt/total,   # 文本读出准确率（zero-shot 风格）\n",
    "}\n",
    "\n"
   ],
   "id": "a289aacb130219b7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:17:27.294309Z",
     "start_time": "2025-10-31T08:16:43.524939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_val = -1.0\n",
    "best_head = None\n",
    "earlystop = EarlyStopper()\n",
    "for ep in range(1,cfg.epochs+1):\n",
    "    training = run_epoch(train_loader, train=True)\n",
    "    val = run_epoch(val_loader, train=False)\n",
    "    print(f\"VPT tuned Model on epoch [{ep}/{cfg.epochs}] \"\n",
    "          f\"Train: loss={training['loss']:.4f} acc_cls={training['acc_cls']:.4f} acc_txt={training['acc_txt']:.4f} | \"\n",
    "          f\"Val:   loss={val['loss']:.4f} acc_cls={val['acc_cls']:.4f} acc_txt={val['acc_txt']:.4f}\")\n",
    "\n",
    "\n",
    "    if val[\"acc_cls\"] > best_val:\n",
    "        best_val = val[\"acc_cls\"]\n",
    "        best_head = { k: v.detach().cpu() for k, v in head.state_dict().items() } # Detach the parameters from autograd (keeps weights only)\n",
    "    earlystop.report(val['loss'])\n",
    "    if earlystop.stop_flag():\n",
    "        print(f\"Early stop triggered...Exiting on epoch {ep}\")\n",
    "        break\n",
    "\n",
    "if best_head is not None:\n",
    "    head.load_state_dict({k: v.to(device) for k, v in best_head.items()})\n",
    "te = run_epoch(test_loader, train=False)\n",
    "print(f\"Test: loss={te['loss']:.4f}  acc_cls={te['acc_cls']:.4f}  acc_txt={te['acc_txt']:.4f}\")"
   ],
   "id": "6dfa3276fbcc3299",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:04<00:00,  7.19it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT tuned Model on epoch [1/40] Train: loss=0.4241 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.3756 acc_cls=0.9176 acc_txt=0.7853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:02<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT tuned Model on epoch [2/40] Train: loss=0.3781 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.3220 acc_cls=0.9186 acc_txt=0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:04<00:00,  7.30it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:02<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT tuned Model on epoch [3/40] Train: loss=0.3382 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.2815 acc_cls=0.9167 acc_txt=0.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:03<00:00,  8.02it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT tuned Model on epoch [4/40] Train: loss=0.3040 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.2646 acc_cls=0.9108 acc_txt=0.7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 32/32 [00:03<00:00,  8.14it/s]\n",
      "Eval: 100%|██████████| 32/32 [00:01<00:00, 16.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPT tuned Model on epoch [5/40] Train: loss=0.2739 acc_cls=1.0000 acc_txt=1.0000 | Val:   loss=1.2214 acc_cls=0.9127 acc_txt=0.7775\n",
      "Early stop triggered...Exiting on epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 193/193 [00:12<00:00, 15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=1.3847  acc_cls=0.9086  acc_txt=0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "273ac7ac8f4d5287"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
